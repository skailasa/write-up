\documentclass[12pt, a4]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\usepackage[backend=bibtex]{biblatex}
\addbibresource{dist.bib}

\title{Linear Partial Differential Equations}
\author{Srinath Kailasa \thanks{srinath.kailasa.18@ucl.ac.uk} \\ \small University College London}

\date{\today}



\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{notation}{Notation}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{problem}[theorem]{Problem}

\DeclareMathOperator\supp{supp}
\DeclareMathOperator\reals{\mathbb{R}}
\DeclareMathOperator\complexes{\mathbb{C}}
\DeclareMathOperator\tfspace{C_0^\infty}
\DeclareMathOperator\tfspaceD{\mathcal{D}}
\DeclareMathOperator\dist{\mathcal{D'}}
\DeclareMathOperator\lone{L_{\text{loc}}^1}
\begin{document}

\maketitle

\section*{Abstract}

In this document I summarise some of the main applications of the theory of distributions for the solution of partial differential equations via the method of `fundamental solutions'. I begin by introducing the key concepts behind the idea of the method of distributions, with the goal of solving the Laplace and Heat equations using these ideas. We proceed to an overview of the Fourier transform, and its usage for solving PDEs, and its description within the theory of distributions. We conclude with a soujourn on Sobolev spaces, and a short description of their utility in extending weak solutions, derived using distribution theory, to full classical solutions of PDEs. Furthermore, this document also includes a `scratch pad' section, in which I provide further examples and proofs of the applications of these ideas.

\section{Distributions and Test Functions}

\subsection{Motivation}

Consider the wave equation in 1D,

\begin{equation}
    \frac{\partial^2u(x, t)}{\partial t^2} = \frac{\partial^2u(x, t)}{\partial x^2}
    \label{eq:wave_eq_1d}
\end{equation}

Any twice continuosly differentiable function $f \in C^2(\reals)$ of the form $u(x, t) = f(x-t)$ satisfies the wave equation. However, we face difficulties if the solution is admissable in a physical sense, but has mathematical problems such as being non-continuous, e.g. $u(x, t) = |x-t|$. This is the motivation behind seeking a new theory, a theory of distributions, which allow us to generalise notions of derivatives and solve problems with solutions that may not not be mathematicallly particlarly `nice' as they may be required by the physics of the problem. These generalisations are known as distributions.

\subsection{Defining Test Functions}

A few basic definitions will serve us well for the remainder of these notes.

\begin{definition}[Classes of Continuous Functions]
Let $\Omega^N$ be an open subset and let $m$ be a non-negative integer. The class $C^m(\Omega)$ consists of functions on $\Omega$ which have continuous derivatives of order less than or equal to $m$. Furthermore (1) $C^0(\Omega) = C(\Omega)$  is simply the class of all continuous functions on $\Omega$ and (2) $C^\infty (\Omega)$ is the class of functions with derivatives of all orders.
\label{def:c_m_functions}
\end{definition}

\begin{definition}[Support of Functions]
The support of a function $f : \Omega \rightarrow \mathbb{C}$ is the closure of the set $\{x \in \Omega | f(x) \neq 0\}$
\[ \supp f = \overline{\{x \in \Omega | f(x) \neq 0\}}\]
\label{def:support_of_functions}
\end{definition}

Examples in $\Omega = \reals$

\begin{itemize}
    \item $f(x) = 0$, then  $\supp f = \emptyset$
    \item $f(x) = x$, then $\supp f = \reals$
\end{itemize}

\begin{definition}
    $C_0^\infty(\reals^N)$ is the subset of $C^\infty(\reals^N)$ consisting of functions with \textbf{compact} support.
\end{definition}

\begin{definition}[Compact]
    Compact in this context means a \textbf{closed} and \textbf{bounded} set.
    \[ \Omega \subset \reals^N\]
    is bounded if $\exists R > 0$ such that
    \[\Omega \subset B_R = \{ x : |x| < R\}\]
    and $|x|$ can be understood as a length in $\reals^N$.
    \label{def:compact_function}
\end{definition}

Now we list some of the basic properties of the function space described by $\tfspace$, as this is the space from which we seek our coveted test functions. We can think of it as a linear vector space, with the usual properties. That is, for $\phi_1, \phi_2 \in \tfspace$ and $\alpha_1, \alpha_2 \in \complexes$,

\begin{eqnarray}
    \alpha_1\phi_1 + \alpha_2\phi_2  \in \tfspace
\end{eqnarray}

Before arriving at the final definition of test functions, we must also introduce the concept of the multi-index. This has a familiar whiff about if for those who are familiar with vector-calculus.

\begin{notation}[Multi-Index]
    A multi-index is a vector with $N$ components, \[ \alpha=(\alpha_1,...,\alpha_N) \] where each component is a non-negative integer. It has an \textbf{order}, described by the sum of all components $|\alpha| = \alpha_1+...+\alpha_N$. If $\beta$ is also a multi-index, then $\alpha+\beta$ is a component-wise sum.
\end{notation}

Using multi-indices we can denote the derivative of a function $f(x_1,x_2,...)$ with respect to a multi-index as,

\begin{eqnarray}
    \partial^\alpha f = \frac{\partial^{|\alpha|}}{\partial x_1^{\alpha_1}, \partial x_2^{\alpha_2}, ..., \partial x_N^{\alpha_N}}
\end{eqnarray}

Furthermore, for $\phi \in \tfspace$ any $\partial^\alpha \phi \in \tfspace$. If $a \in C^\infty$ i.e. a doesn't have compact support, $a\phi \in \tfspace$ - its product with a function with compact support will also have compact support. Finally changes of variables, will also leave us within $\tfspace$ i.e. for an arbitrary vector $b \in \reals^N$ and an $N \times N$ matrix $A$, $\phi(Ax+b) \in \tfspace$.

Now we have a promising vector space, with useful properties for it's members such as compactness and boundedness. There is however one more thing we need to consider, which is the idea of convergence within the vector space.

\begin{definition}
    A sequence $\{\phi_i\}^\infty_{i=1} \in \tfspace $ is said to converge to zero if
    \begin{enumerate}
        \item There exists a compact set $K \subset \reals^N$ such that $\supp \phi_j \subset K$ for all $i=1,2,....$
        \item For each multi-index $\alpha$ the derivative $\partial^\alpha \phi_j$ converges to zero uniformly as $j \rightarrow \infty$. That is, for all $\alpha$, $\underset{x \in \reals^N}{\supp} |\partial^\alpha \phi_j(x)|\rightarrow 0$
    \end{enumerate}

    \label{def:convergence_of_tf}
\end{definition}


The set $\tfspace$ with the above convergence properties is called the \textbf{space of test functions}, and denoted by $\tfspaceD$. We remark that the convergence property is often denoted with respect to $\tfspaceD$ in the literature as,

\begin{eqnarray}
    \phi_N \overset{\tfspaceD}{\rightarrow} \phi
\end{eqnarray}

which further implies,

\begin{eqnarray}
    \phi_N - \phi \overset{\tfspaceD}{\rightarrow} 0
\end{eqnarray}

\subsection{Defining Distributions}

Consider two abstract sets $A$ and $B$, the map between them $f : A \rightarrow B$ defines a \textbf{functional}. The map $f$ is considered to be continuous if $a_n \overset{A}{\rightarrow} a$ implies that $f(a_n) \overset{B}{\rightarrow} f(a)$. Strictly, this is `sequential continuity'.

\begin{definition}
    The set of distributions, $\dist$, is the set of all \textbf{linear} and \textbf{ (sequentially) continuous} functionals that map $f : \tfspaceD \rightarrow \complexes$
\label{def:distribution}
\end{definition}

We see that the distributions are defined by mapping from the space of test functions to (potentially complex) numbers. This concept will allow us to generalise notions of differentiability.

If $f$ is a functional, then

\begin{enumerate}
    \item For each $\phi \in \tfspaceD$, the functional associates a number denoted by the following Bra-Ket notation $\langle f, \phi \rangle \in \complexes$. This is also described as the \textbf{action} of the functional.
    \item The functional is linear such that for $\alpha_1, \alpha_2 \in \complexes$ and $\phi_1, \phi_2 \in \tfspaceD$, $\langle f, \alpha_1\phi_1, \alpha_2\phi_2 \rangle = \alpha_1\langle f, \phi_1 \rangle + \alpha_2\langle f, \phi_2 \rangle $
    \item A distribution $f$ is called (sequntially) continuous on $\tfspaceD$ if $\langle f, \phi_k \rangle \rightarrow \langle f, \phi \rangle$ where $\phi_k \overset{\tfspaceD}{\rightarrow} \phi$ as $k \rightarrow \infty$
\end{enumerate}

Let us illustrate the concept of functionals which satisfy the definition of distributions by going through some examples.

\begin{enumerate}
    \item \textbf{Continuous functions as distributions}. Let $f \in C(\reals^N)$ and define a functional with the mapping $\tfspaceD(\reals^N) \rightarrow \complexes$ as $\langle f, \phi \rangle = \int_{\reals^N} f(x) \phi(x) dx < +\infty$ which is valid for $\forall \phi \in \tfspaceD$. The integral is linear, by inspection, so it remains to show that it is also continuous in the sense of sequential continuity. Consider a sequence of test functions $\phi_n \in \tfspaceD$ such that $\phi_n \rightarrow \phi$ in $\tfspaceD$, without loss of generality let us assume that $\phi = 0$ so we have to show now that the action $\langle f, \phi_n \rangle \rightarrow 0$. From the definition of convergence in $\tfspace$ (\ref{def:convergence_of_tf}), we know that there exists a compact set $K$ for which $\supp \phi_n \subset K \> \> (\forall n)$. Then, \[ |\langle f, \phi_n \rangle | = \left |\int_{\reals^N}f \phi_n dx \right | = \left |\int_K f \phi_n dx \right | \leq \sup_{x \in \reals^N} |\phi_n| \int_K |f| dx\]. From (\ref{def:convergence_of_tf}) we see that $\sup_{x \in \reals^N} |\phi_n| \rightarrow 0$ and the integral over $|f|$ in $K$ is finite due to the compactness of $K$, therefore $\langle f, \phi_n \rangle \rightarrow 0$ and the functional is sequentiallly continuous. A remark about notation, we've identified above a function $f$ with a \textit{functional} which we also denote by $f \in \dist$.
    \item \textbf{Heaviside function}. It therefore follows from above that a function defines a distribution if the following statement holds, \[ \int_K |f| dx < + \infty, \> \> \forall K \subset \reals^N \]

    Consider the Heaviside function in one dimension, \[ H(x) = \begin{cases}
        1, x > 0 \\
        0, x \leq 0
    \end{cases}\]
    Then we can define the action of the distribution generated by the Heaviside function as, \[ \langle H, \phi \rangle = \int_{-\infty}^{\infty} H(x) \phi(x) dx = \int_{0}^{\infty} \phi(x) dx, \> \> \forall \phi \in \tfspaceD \]

    Therefore we see that it also identifiable as a distribution.
\end{enumerate}

Before proceeding, we need the concept of regular distributions,

\begin{definition}[Regular Distribution]
A distribution $f \in \dist$ is called regular if $\exists f \in \lone$ such that \[ \langle f, \phi \rangle = \int_{\reals^N} f(x) \phi(x) dx, \> \> \forall \phi \in \tfspaceD\]
    \label{def:regular_dist}
\end{definition}

Here $\lone$ defines a function space where each member is `locally integrable', i.e. has a finite integral on every compact subset of its domain of definition. More formally, let $\Omega$ be an open set in $\reals^N$ and $f : \Omega \rightarrow \complexes$ if the following integral holds,
\begin{equation}
    \int_K |f| dx < + \infty
    \label{eq:l_one_loc}
\end{equation}

for all subsets $K \subset \Omega$, then $f \in \lone$. The 1 refers to the power of the absolute value in the integral. We then notice that the conditions for sequential continuity are satisfied if a functional is chosen from $\lone$.

\vspace{5pt}

As a final example of this section, consider the Dirac $\delta$-function, defined by the following action,

\begin{eqnarray}
\langle \delta, \phi \rangle = \phi(0), \> \> \forall \phi \in \tfspaceD(\reals^N)
    \label{eq:dirac}
\end{eqnarray}

The functional is clearly in $\lone$, thefore satisfies sequential continuity, with linearity obvious by inspection. Therefore we see that it satisfies the properties required of a distribution.

\subsection{Basic Operations on Distributions}

Here, we state some basic facts about operations on distributions largely without proof.

\subsubsection{Multiplication by a $C^\infty$ function}
For example, If $a \in C^\infty(\reals^N)$ and $f \in C(\reals^N)$ then $f \in \dist$ then,

\begin{equation}
    \langle af, \phi\rangle = \int_{\reals^N}(a(x)f(x))\phi(x)dx = \int_{\reals^N}f(x) (a(x)\phi(x))dx = \langle f, a\phi \rangle
\end{equation}

Implying that $a\phi \in \tfspace$. This identity can be extended to all distributions.

\begin{definition}
    If $a \in C^\infty$ and $f \in \dist$ then, $af \in \dist$ defined by \[
        \langle af, \phi \rangle = \langle f, a\phi \rangle, \>\> \forall \phi \in \tfspaceD\]
\end{definition}

One can check that the above does define a distribution through an example. Consider $f = \delta \in \dist$ and $a \in \tfspace$, then

\begin{eqnarray}
    \langle a\delta, \phi \rangle = \langle \delta, a \phi \rangle = a(0)\phi(0) = a(0) \langle \delta, \phi \rangle \> \> \forall \phi \in \tfspaceD
\end{eqnarray}

This implies that $a\delta = a(0) \delta$ and does define a distribution.

\subsubsection{Differentiation}

Differentiation for distributions uses a form of Green's identity. Consider the one dimensional case, $f \in C(\reals)$ and $\phi \in \tfspaceD(\reals)$ and has compact support, then

\begin{equation}
    \int_{-\infty}^{\infty} f'(x) \phi(x) dx = \int_a^b f'(x)\phi(x)dx = \left [ f \phi \right ]_a^b - \int_a^b f\phi' dx = -\int_a^b f\phi' dx
\end{equation}

The boundary term vanishes due to the compact support of $\phi$. Generalising the above identity for higher order derivatives, we see that,

\begin{equation}
    \int_{-\infty}^{\infty} \frac{d^m}{dx^m} f(x) \phi(x) dx = (-1)^m \int_{-infty}^{\infty} f(x) \frac{d^m}{dx^m}\phi(x) dx
\end{equation}

In the language of actions this goes to,

\begin{eqnarray}
    \langle \frac{d^m}{dx^m}f, \phi \rangle = \langle f, \frac{d^m}{dx^m} \phi \rangle
\end{eqnarray}

Furthermore, we state that this differentiated object defines a distribution too.

\begin{definition}
    Let $f \in \dist(\reals^N)$. Then for any multi-index $\alpha$ the distribution $\partial^\alpha f$ is defined to be, \[ \langle \partial^\alpha f, \phi \rangle = (-1)^{|\alpha|} \langle f, \partial^\alpha \phi \rangle, \> \> \forall \phi \in \tfspaceD(\reals^N) \]
    \label{def:differentiate_distribution}
\end{definition}

Let's consider some simple corollary examples,

\begin{enumerate}
    \item Consider a one dimensional problem in $\reals$ where $\delta \in \dist$. What is $\frac{d}{dx}\delta$? From the above definition, \[
        \langle \frac{d}{dx} \delta, \phi\rangle = - \langle \delta, \frac{d}{dx}\phi \rangle = -\phi'(0)\]
    \item Consider again a one dimension problem with the Heaviside function $H \in \dist$. What is $\frac{d}{dx}H$? Using the same procedure, \[
        \langle \frac{d}{dx}H, \phi \rangle = -\langle H, \frac{d}{dx} \phi \rangle = \int_0^\infty \frac{d\phi}{dx}dx = -[\phi]_0^\infty = \phi(0) = \langle \delta, \phi \rangle, \> \> \forall \phi \in \tfspaceD \]. This implies that $\frac{dH}{dx} = \delta$, in a distributional sense.
\end{enumerate}

\subsubsection{Change of Variables ($x = Ay + b$)}

Finally let's consider a linear transformation, corresponding to a change of variables. Here $A$ is a non-singular $N \times N$ matrix and $b \in \reals^N$. If $f \in C^\infty(\reals^N)$ and $\phi \in \tfspaceD(\reals^N)$, then

\begin{equation}
    \int_{\reals^N}f(Ay+b)\phi(y)dy = \int_{\reals^N}f(x)\phi(A^{-1}(x-b))\frac{1}{\text{det}(A)}dx
\end{equation}

This also defines a distribution, giving us our final definition in this section,

\begin{definition}
    Let $f \in \dist(\reals^N)$, with $A$ and $b$ as above, then $f(Ay+b) \in \dist$ is defined by \[\langle f(Ay+b), \phi(y)\rangle = \langle f(x), \phi(A^{-1}(x-b))\frac{1}{|A|}\rangle, \> \> \forall \phi \in \tfspaceD(\reals^N)\]
\end{definition}

\subsection{The Support of Distributions}

Distributions do not take `values' in a domain like a function would, but we define an equivalent statement by using their action. For example for a distribution $f$, the statement $f=0$ is read to mean that $\langle f, \phi \rangle = 0, \> \> \forall \phi \in \Omega, \> \> s.t. \> \> \supp \phi \subset \Omega$.

\begin{remark}
    For $f, g \in \dist$, then $f=g$ in $\Omega$ means that, \[ \langle f, \phi \rangle = \langle g, \phi \rangle, \> \> \forall \phi \in \tfspaceD(\Omega)\]
\end{remark}

We can now use this technology to define the support of a distribution.

\begin{definition}
    Let $f \in \dist(\reals^N)$. Then $\supp f$ is the complement of the set \[ O_f = \left \{ x \in \reals^n : f = 0 \> \> \text{on a neighbourhood of x}  \right \} \] This $O_f$ is known as the zero-set of $f$.
\end{definition}

\subsection{The Convergence of Distributions}

Recalling our sequential continuity condition, let $\{f_j\}_{j=1}^\infty$ be a sequence of distributions on $\reals^N$ where $f \in \dist(\reals^N)$.

\begin{definition}
    We say that $f_n$ converges to $f$ in $\dist$, and write $f_n \overset{\dist}{\rightarrow} f$, if \[ \langle f_n, \phi \rangle \rightarrow \langle f, \phi \rangle, \> \> \forall \phi \in \tfspaceD \] as $n \rightarrow \infty$
\end{definition}

Consider $f_n(x) = \frac{1}{n} \sin(nx)$. Clearly $f_n \in \lone(\reals)$, therefore $f_n$ defines a distribution, with the action

\begin{equation}
    \langle f_n, \phi \rangle = \int_{\reals} f_n(x) \phi(x) dx
\end{equation}

Indeed it's obvious that $f_n \rightarrow 0$ in the sense of $\dist$, noticing that

\begin{equation}
    | \langle f_n, \phi \rangle | = \left | \int_{\reals} \frac{1}{n} \sin(nx) \phi(x) dx\right | \leq \frac{1}{n} \left | \int_{\reals}\phi(x) \right |
\end{equation}

This leads to the following proposition. If $f_n \overset{\dist}{\rightarrow} f$, then $\partial^\alpha f_n \overset{\dist}{\rightarrow} \partial^\alpha f$ for all multi-indices $\alpha$. Consider a corollary example, where $g_n(x) = \cos(nx)$, then $g_n \overset{\dist}{\rightarrow} 0 $ as $\frac{d}{dx}(\frac{1}{n}\sin(nx)) = g_n$ and therefore we can apply the above result.

\vspace{5pt}

For a more involved example, consider $f \in \lone(\reals)$ and let $\supp f \in [-1, 1]$ with $\int_{-1}^1 f(x) dx = 1$ and consider the sequence defined by $f_k(x) = kf(kx)$ where $k=1,2,3...$ . Let us show that $f_k \overset{\dist}{\rightarrow} \dist$. Equivalently we can show that there is no difference between the actions of both functionals.

\begin{flalign}
    &\left | \langle f_k, \phi \rangle - \langle \delta, \phi \rangle \right | \\
    &= \left | \int_{-\infty}^\infty f_k(x)\phi(x)dx - \phi(0) \right | \\
    &= \left | \int_{-\infty}^\infty kf(kx)\phi(x)dx - \phi(0)\int_{-\infty}^\infty f(x)dx \right | \\
    &= \left | \int_{-1}^{1}f(x) ( \phi(\frac{x}{k}) - \phi(0) ) dx \right| \\
    &\leq \max_{x \in [-1, 1]} | \phi(\frac{x}{k}) - \phi(0)| = \max_{t \in [-1/k, 1/k]} | \phi(t) - \phi(0)| \rightarrow 0
\end{flalign}

These examples lead us to the final theorem of this section,

\begin{theorem}[Completeness of $\dist$]
    Consider the sequence of distributions $f_1,...,f_n,...$ such that for all $\phi \in \tfspaceD$ the sequence $\langle f_k, \phi \rangle$ converges as $k \rightarrow \infty$. Then the functional defined by, \[ \langle f, \phi \rangle = \lim_{k \rightarrow \infty} \langle f_k, \phi \rangle \] is linear and continuous, i.e it defines a distribution $f \in \dist$.
\end{theorem}

The intuitive interpretation of this theorem is basically that we can't leave the space of distributions by summing distributions.

\subsection{Equivalent Definition of Distributions}

\begin{notation}
    If $\Omega$ is an open set in $\reals^N$ and $K$ is a compact subset of $\Omega$ and $u \in C^m(\Omega)$ then \[ \| u \|_{C^m(K)} = \max_{x \in K, \> |\alpha| \leq m} |\partial^\alpha u| \]
\end{notation}

This allows us to write down our equivalent formulation of distributions.

\begin{definition}
    A linear functional $f$ on $\tfspaceD(\reals^N)$ is a distribution iff for all compact sets $K$ in $\reals^N$ there exist $C$ and $m$ such that,
    \[ |\langle f, \phi \rangle| \leq C \| \phi \|_{C^m(k)}\] For all $\phi \in \tfspaceD$ such that $\supp \phi \subset K$.
\end{definition}

Proving the reverse implication. We have the above definition, and we need to show that it satisfies sequential continuity. That is if $\phi_n \overset{\tfspaceD}{\rightarrow} \phi$, then $\langle f, \phi_n \rangle \overset{\dist}{\rightarrow} 0$

\vspace{5pt}

Proving the forward implication. Assume that $f \in \dist$, then let's prove by contradiction. Assume the alternative definition doesn't hold,

\begin{eqnarray}
    | \langle f, \phi_m \rangle | > m \| \phi_m \|_{C^m(K)}
\end{eqnarray}

Define $\psi_m = \frac{\phi_m}{m \| \phi_m \|_{C^m(K)}}$. Then $\psi_m \in \tfspaceD$, $\supp \psi_m \subset K$ and for any multi-index $\beta$ such that $m \geq |\beta|$, we have,

\begin{eqnarray}
    \| \psi_m \|_{C^{|\beta|}(K)} = \frac{\|\phi_m\|_{C^{|B|}(K)}}{m \|\phi_m\|_{C^m(K)}} \leq \frac{1}{m}
\end{eqnarray}

Consequently $\psi_m \overset{\tfspaceD}{\rightarrow} 0$. However, this contradicts our initial assumption.

\section{Fundamental Solution of Laplacian in $\reals^3$}

For a linear partial differential equation with linear operator $L$,

\begin{eqnarray}
    LF = \delta(x)
\end{eqnarray}

the fundamental solution, $F$,  is defined as that which yields the $\delta$-function up to a constant, and is therefore defined as the solution of the above equation.

Consider the Poisson problem,

\begin{eqnarray}
    \Delta u = f
    \label{eq:laplace}
\end{eqnarray}

Where the linear partial differential operator, the Laplace operator, is given by

\begin{eqnarray}
    \Delta = \sum_{i=1}^N \frac{\partial^2}{\partial x_i^2}
\end{eqnarray}

Let's check that the fundamental solution in $\reals^3$ is described by the following relation,

\begin{equation}
    \Delta \frac{1}{|x|} = -4\pi \delta(x)
\end{equation}

where as usual $|x| = \sqrt{x_1^2+x_2^2+x_3^2}$. We can check that the Laplacian of the fundamental solution is 0 in $\reals^3 \backslash \{ 0\}$ using our ordinary notions of derivative,

\begin{eqnarray}
    \frac{\partial}{\partial x_1} \frac{1}{|x|} = -\frac{x_1}{(x_1^2+x_2^2+x_3^2)^{\frac{3}{2}}}
\end{eqnarray}

and,

\begin{eqnarray}
    \frac{\partial^2}{\partial x_1 ^2} \frac{1}{|x|} = \frac{3x_1^2}{(x_1^2+x_2^2+x_3^2)^{\frac{5}{2}}}-\frac{1}{(x_1^2+x_2^2+x_3^2)^{\frac{3}{2}}}
\end{eqnarray}

from which the result follows. It's not really possible to define derivatives at the origin for our fundamental soltuion in the usual sense, however we can show that $1/|x| \in \lone$ - this would allow us to define it as a distribution with all of the useful properties that they have. To see that the statement is true, we simply have to observe that the integral,

\begin{eqnarray}
    \int_{K} \frac{1}{|x|} dx < +\infty
\end{eqnarray}

for all compact subsets $K \subset \reals^3$. If this is the case, then let's take a ball such that $K \subset B_R = \{ x | x < R\}$. Then,

\begin{flalign}
    \int_K |f| dx \leq \int_{B_R} |f| dx \leq \int_0^R \int_{S_r} |f| d\omega dr
\end{flalign}

where we've switched to using spherical coordinates defined by the surface of a sphere $\omega \in S_r = \{\ x: \sqrt{x_1^2 + x_2^2 + x_3^2}\}$, and the radial coordinate $r \in [0, R]$

\begin{flalign}
    &\int_0^R \frac{1}{r} \int_{S_r} d\omega dr \\
    &\int_0^R \frac{1}{r} r^2 dr = \int_0^R r dr < +\infty
\end{flalign}

This result extends to $\reals^N$ too, and allows to say that $1/|x| \in \lone$ and therefore that $1/|x| \in \dist$ and is a regular distribution.
Therefore, let's now take derivaitives of the fundamental solution in a distributional sense,

\begin{equation}
\langle \Delta \frac{1}{|x|}, \phi \rangle = \langle \frac{1}{|x|}, \Delta \phi \rangle = \int_{\reals^3} \frac{1}{|x|} \Delta \phi dx
\end{equation}

This is of course for any $\phi \in \tfspaceD(\reals^3)$. We note that the test functions have a finite support, $\supp \phi \subset B_R$, where we've drawn a ball around the compact subset $K$ in which they are supported to help our integral evaluation,

\begin{flalign}
 \int_{B_R} \frac{1}{|x|} \Delta \phi dx = \lim_{\epsilon \rightarrow 0} \int_{B_R \backslash \bar{B_\epsilon}} \frac{1}{|x|} \Delta \phi dx
\end{flalign}

Here we're taking the integral over the ball, and excluding an ever decreasing ball that is centered over the origin of radius $\epsilon$. Let's now integrate using Green's second identity,

\begin{flalign}
    \lim_{\epsilon \rightarrow 0} \left( \int_{B_R \backslash \bar{B}_\epsilon} \Delta \frac{1}{|x|} \phi dx + \oint_{S_\epsilon} \partial_n \phi \frac{1}{|x|} ds  - \oint_{S_\epsilon} \phi \partial_n \frac{1}{|x|} ds \right)
\end{flalign}

Here we've skipped some steps in the surface integrals, specifically we note that they will be zero over the outer surface due to the compact support of the test functions, and therefore exclude them from the integral. Furthermore, the first integral is zero by definition as we've shown that the Laplacian of the fundamental solution is 0 when excluding the origin.

\vspace{5pt}

We are then left with the two surface integrals,

\begin{flalign}
    \lim_{\epsilon \rightarrow 0} \left( \oint_{S_\epsilon} \partial_n \phi \frac{1}{|x|} ds  - \oint_{S_\epsilon} \phi \partial_n \frac{1}{|x|} ds \right)
\end{flalign}

Consider the first one of these,

\begin{flalign}
    \left | \oint_{S_\epsilon} \partial_n \phi \frac{1}{|x|} ds \right | &\leq \oint_{S_\epsilon} \max_{S_\epsilon}  | \nabla \phi| \frac{1}{|x|} ds \\
    & \leq \| \phi \|_{C^1(\reals^3)} \oint_{S_\epsilon} \frac{1}{|x|} ds \\
    & = \| \phi \|_{C^1(\reals^3)} \frac{1}{\epsilon} \oint_{S_\epsilon} ds = \| \phi \|_{C^1(\reals^3)} 4\pi \epsilon \overset{\epsilon \rightarrow 0}{\rightarrow} 0
\end{flalign}

Now the second, using the fact that the unit normal vector over the inner surface (pointing towards the origin) is $\hat{n}= -\frac{x}{|x|}$ and therefore that $\partial_n = -\partial_r$

\begin{flalign}
    &\oint_{S_\epsilon} \phi \partial_r \frac{1}{r} ds = - \oint_{S_\epsilon} \phi \frac{1}{r^2}ds = -\frac{1}{\epsilon^2} \oint_{S_\epsilon} \phi(x)ds \\
    & = -\frac{1}{\epsilon^2} \oint_{S_\epsilon} \phi(x) - \phi(0) ds - \frac{1}{\epsilon^2} \oint_{S_\epsilon} \phi(0) ds \\
    & = -\frac{1}{\epsilon^2} \oint_{S_\epsilon} \phi(x) - \phi(0) ds - 4\pi \phi(0)
\end{flalign}

All that remains is to show that the first integral converges to 0 and we'll have our result,

\begin{flalign}
    &\left | \frac{1}{\epsilon^2} \int_{S_\epsilon} \phi(x) - \phi(0) ds \right | \\
    & \leq \frac{1}{\epsilon^2} \int_{S_\epsilon} \left | \phi(x) - \phi(0)\right | ds \leq 4\pi \max_{x \in S_\epsilon} | \phi(x) - \phi(0) | \rightarrow 0
\end{flalign}

Therefore we see,

\begin{flalign}
    \langle \Delta \frac{1}{|x|}, \phi \rangle = -4\pi \phi(0) = -4\pi \langle\delta, \phi \rangle
\end{flalign}

for all $\phi \in \tfspaceD$, and that $\Delta \frac{1}{|x|} = -4\pi \delta$. Note that the fundamental solution is a family of solutions, and is not unique up to a constant.

The name `fundamental solution' seems very important, and as we shall see we will be able to write general solutions of PDEs using knowledge of them. However, in order to get to that point we need to have a few more operations on distributions up our sleeves.

\section{Direct Product of Distributions}

Let's take a series of steps which are as of yet poorly defined, but motivate the need for more operations on distributions. Consider again the Poisson problem in $\reals^3$, we now know the fundamental solution for this problem,

\begin{flalign}
E(x) = -\frac{1}{4\pi}\frac{1}{|x|}
\end{flalign}

Let's displace the solution by some fixed vector $y$, the Poisson problem then goes to,

\begin{flalign}
    \Delta_xE(x-y) = \delta(x-y)
\end{flalign}

Now, for our first poorly defined operation, multiplication by a distribution,

\begin{flalign}
    f(y)\Delta_xE(x-y) = f(y)\delta(x-y)
\end{flalign}

And then our second poorly defined operation, integration over the domain with respect to this fixed vector $y$

\begin{flalign}
    \int f(y)\Delta_xE(x-y) dy = \int f(y)\delta(x-y) dy
\end{flalign}

By linearity, and using the properties of the $\delta$-function,

\begin{flalign}
    \Delta_x \int f(y)E(x-y) dy = f(x)
\end{flalign}

We see that our Poisson problem is `solved' by,

\begin{flalign}
    u(x) = \int f(y)E(x-y) dy
\end{flalign}

meaning that we can derive a weak solution of the Poisson problem with knowledge of just the fundamental solution - a hint as to why these objects are so fundamental, they are in some way atomistically tied to a more general solution of the differential equation. As we shall see they can do even better than define weak solutions. However, in order to rigorously use this intuition, we need to first learn how distributions can be in a sense `multiplied' by each other. This motivates the desire to define a `direct product' between distributions, which generalise notions of multiplications to distributions.

Let us start with defining direct products with respect to test functions in $\reals^N$. Consider $f,g \in \tfspaceD(\reals^N)$ where the product $fg \in \tfspaceD(\reals^N)$. We define the tensor or direct product as $f \otimes g \in \tfspaceD(\reals^{N+N})$ in some `dual' space. As we can describe continuous functions as distributions, it is possible to write,

\begin{flalign}
    \langle f(x)g(y), \phi(x, y) \rangle &= \int_{\reals^{N+N}} f(x)g(y)\phi(x,y)dxdy \\
    &= \int_{\reals^N} f(x) \left( \int_{\reals^N} g(y) \phi(x, y) dy\right) dx \\
    &= \langle f(x), \langle g(y), \phi(x, y) \rangle_y \rangle_x
\end{flalign}

Now we can write down the definition of direct products between distributions,

\begin{definition}[Direct Product of Distributions]
    Let $f \in \dist(\reals^N)$ and $g \in \dist(\reals^M)$. Then $f \otimes g \in \dist(\reals^{N+M})$ defined by \[ \langle f \otimes g, \phi \rangle= \langle f(x), \langle g(y), \phi(x, y) \rangle \rangle \] valid for all $\phi \in \tfspaceD(\reals^{N+M})$
    \label{def:direct_product}
\end{definition}

If we check that the RHS of the above identity defines a linear and continuous functional, and therefore a valid distribution, then it is valid. Let's write it down in a lemma,

\begin{lemma}
    Let $g \in \dist(\reals^M)$ and $\phi \in \tfspaceD(\reals^{N+M})$ then $\psi(x) = \langle g(y), \phi(x, y) \rangle \in \tfspaceD(\reals^N)$ additionally \[ \partial^\alpha \psi(x) = \langle g(y), \partial^\alpha_x\phi(x, y)\rangle\] Furthermore, if $\phi_k \rightarrow 0$, then \[ \psi_k(x) = \langle g(y), \phi_k(x, y) \rangle \rightarrow 0\] in the space $\tfspaceD(\reals^N)$
\end{lemma}

All this is basically saying is that the inner action describes a valid test function for the outer action.

\vspace{5pt}

Let's begin by showing that $\psi$ is continuous. It is enough to show this in one direction, as the same technique can be used in turn to show it for other dimensions too. Let's fix $x_0 \in \reals^N$, and consider a sequence such that $x_k \rightarrow x_0$ with $x_k \in \reals^N$. The support of the test function can be seen to be fixed within some ball $\supp \phi(x_k, y) \subset B_b$ and from linearity, we know that for any multi-index $\beta$ that $\partial_y^\beta \phi(x_k, y) \rightarrow \partial_y^\beta\phi(x_0, y)$ uniformly in $y$. Which, due to the continuity of $g(y)$ gives,

\begin{eqnarray}
    \psi(x_k) = \langle g(y), \phi(x_k, y)\rangle \rightarrow \langle g(y), \phi(x_0, y) \rangle
\end{eqnarray}

Thus $\psi$ is sequentially continuous. All that remains is to show the two properties of the lemma.

To prove the first property, let's fix $x_0 \in \reals^N$ and take the derivative along just a single axis $e_1 = (1,0,0,...,0)$, then from the fundamental theorem of calculus,

\begin{flalign}
    \frac{\partial}{\partial x_1}\psi(x_0) &=\lim_{\epsilon \rightarrow 0} (\psi(x_0 + \epsilon e_1) - \psi(x_0)) \\
    & =\lim_{\epsilon \rightarrow 0} \left(\langle g(y), \frac{\psi(x_0 + \epsilon e_1) - \psi(x_0)}{\epsilon} \rangle \right)\\
    & = \lim_{\epsilon \rightarrow 0} \left( \langle g(y), \mu_\epsilon(y) \rangle \right)
\end{flalign}

We notice that $\mu_\epsilon(y) \rightarrow \frac{\partial \phi}{\partial x_1}(x_0, y)$ in $\tfspaceD(\reals^M)$, thus validating the first property in the lemma in a single dimension - this can be repeated for any dimension or set of dimensions.

\vspace{5pt}

Finally, let's show the second statement in the above lemma. If $\phi \rightarrow 0$ in $\tfspaceD(\reals^{N+M})$ then $\exists a, b$ such that $\supp \phi_k \subset B_a \times B_b$ then $\supp \psi_k(x) = \supp \psi(x_k, \cdot) \subset B_a$. We need to show that $\forall \alpha$

\begin{eqnarray}
    \sup_{x \in \reals^N} |\partial_x^\alpha \psi_k(x) | \rightarrow 0
\end{eqnarray}

Let's prove by contradiction. If it's not true, then $\exists \alpha_0, x_k, \epsilon_0 > 0$ such that,

\begin{eqnarray}
    |\partial_x^{\alpha_0} \psi_k(x) | \geq \epsilon_0
\end{eqnarray}

Consider a function $f_k(y) = (\partial_x^{\alpha_0}\phi_k)(x_k, y)$ for any multi-index $\beta$ we know that ,

\begin{eqnarray}
    \partial_x^{\alpha_0} \partial_y^\beta \phi_k (x, y) \rightarrow 0
\end{eqnarray}

uniformly in $x$ and $y$ simply from the peroperties of test functions. We can parametrize for a sequence $x_k$,

\begin{eqnarray}
    \partial_x^{\alpha_0} \partial_y^\beta \phi_k (x_k, y) \rightarrow 0
\end{eqnarray}

uniformly in $y$ as $k \rightarrow \infty$. By linearity, we can swap the order of the differential operators,

\begin{flalign}
    &\partial_y^\beta f_k(y) \rightarrow 0 \\
    & f_k \rightarrow 0
\end{flalign}

in the sense of $\tfspaceD(\reals^M)$. This implies that,

\begin{flalign}
    \langle g(y), f_k(y) \rangle \rightarrow 0
\end{flalign}

which leads to a contradiction, thereby proving our lemma.

\begin{corollary}[The Map A]
    $\phi \in \tfspaceD(\reals^{N+M})$ mapped by $A\phi = \psi(x) = \langle g(y), \phi(x, y) \rangle$ is linear and continuous from $\tfspaceD(\reals^{N+M})$ to $\tfspaceD({\reals^N})$.
\end{corollary}

Now that we've defined products on distributions, let's examine some basic properties that will be useful for us,

\subsubsection*{1. Commutativity}

If $f \in \dist(\reals^N)$ and $g \in \dist(\reals^M)$ then $f(x) \otimes g(y) = g(y) \otimes f(x)$. Let's show this by considering the following dual test function $\phi(x, y) = \underset{1 \leq j \leq T}{\sum}u_j(x)v_j(y)$ where $u_j \in \tfspaceD(\reals^N)$ and $v_j \in \tfspaceD(\reals^M)$. Then using linearity and the properties of the direct product,


\begin{flalign}
    \langle f(x) \otimes g(y), \phi \rangle &= \underset{1 \leq j \leq T}{\sum} \langle f(x), \langle g(y), u_j(x)v_j(y) \rangle_y \rangle_x \\
    &= \underset{1 \leq j \leq T}{\sum} \langle f(x), u_j(x) \langle g(y), v_j(y) \rangle \rangle \\
    &= \underset{1 \leq j \leq T}{\sum} \langle f(x) , u_j(x) \rangle\langle g(y) , v_j(y) \rangle \\
    &= \underset{1 \leq j \leq T}{\sum} \langle g(y), \langle f(x), u_j(x) \rangle_x v_j(y) \rangle_y \\
    &= \underset{1 \leq j \leq T}{\sum} \langle g(y), \langle f(x), u_j(x)v_j(y) \rangle\rangle \\
    &= \langle g(y) \otimes f(x), \phi \rangle
\end{flalign}

To extend this to \textbf{any} test functions, and not just those of the special form above, we need to show that the set of test functions of that form are \textbf{dense} in $\tfspaceD(\reals^{N+M})$

\begin{definition}[Dense]
    $\forall \psi \in \tfspaceD(\reals^{N+M}) \exists \phi_n \in \tfspaceD(\reals^{N+M})$ of the form $\phi_n(x, y) = \underset{1 \leq j \leq T}{\sum}u_{nj}(x)v_{nj}(y)$ such that $\phi_n\overset{\tfspaceD}{\rightarrow}\psi$
\end{definition}

We basically have to prove that a mapping exists using a sequence of test functions of the special form, to any test function in the space of test functions. If this is the case, then our commutativity result above applies to arbitrary test functions from the space of test functions.

\begin{theorem}[Weierstrass Theorem]
    If $\tilde{\Omega}$ is a bounded domain and $\bar{\Omega} \subset \tilde{\Omega}$ then $\forall f \in C^m(\tilde{\Omega})$ $\forall \epsilon > 0$ $\exists$ a polynomial $P$ such that \[ \| \partial^\alpha f - \partial^\alpha P\|_{C(\Omega)} < \epsilon\] $\forall |\alpha| \leq m$
    \label{thm:weierstrass}
\end{theorem}

The above theorem basically tells us that we can approximate our test functions and their derivatives using polynomials to an arbitrary degree of precision. Why do we need this? Well, it means that if we can approximate our test function as a sequence of polynomials, which can always be written in the nice separable form that allows us to use our commutativity result for arbitrary test functions. Thus we arrive at the following lemma,

\begin{lemma}
    $\forall \phi \in \tfspaceD(\reals^{N+M})$ there exists a sequence of test functions from $\tfspaceD(\reals^{N+M})$ of the form \[\phi_n(x, y) = \underset{1 \leq j \leq T}{\sum}u_{nj}(x)v_{nj}(y)\]  where $u_{jn} \in \tfspaceD(\reals^N)$ and $v_{jn} \in \tfspaceD(\reals^M)$ which converges to $\phi$ in the sense of $\tfspaceD(\reals^{N+M})$.
\end{lemma}

Let's now apply it in a proof. Suppose that $\supp \phi \subset B_R \times B_R$. Then by Weierstrass' theorem $\exists$ a sequence of polynomials $P_k(x, y)$ with $k=1,2,...$ such that

\begin{equation}
    |\partial^\alpha \phi(x, y) - \partial^\alpha P_k(x, y) | < \frac{1}{k}
\end{equation}

where $|\alpha| \leq k$ in a larger domain, let's say $B_{3R} \times B_{3R}$, let's pick two convenient test functions, $\zeta \in \tfspaceD(\reals^N)$ and $\eta \in \tfspaceD(\reals^M)$ with the properties that they are equal to 1 inside a domain of $B_{2R}$ and equal to 0 outside of it. Then, we obstain a sequence that satisfies the compact support property of test functions, and is separable as required to show our lemma,

\begin{eqnarray}
    \phi_k(x, y) = \zeta(x)\eta(y)P_k(x, y)
\end{eqnarray}

as $\supp \phi_k \subset B_{2R} \times B_{2R}$ and $\forall \alpha$ and $k \geq |\alpha|$. We can then say that $\phi_k \overset{\tfspaceD}{\rightarrow} \phi$. So, as we can approximate a test function as a sequence of polynomials, the commutativity property follows for general test functions,

\begin{flalign}
    \langle f(x) \otimes g(y), \phi \rangle &= \lim_{k \rightarrow \infty}  \langle f(x) \otimes g(y), \phi_k \rangle \\
    & = \lim_{k \rightarrow \infty}  \langle g(y) \otimes f(x), \phi_k \rangle \\
    & = \langle g(y) \otimes f(x), \lim_{k \rightarrow \infty} \phi_k \rangle \\
    & = \langle g(y) \otimes f(x), \phi \rangle
\end{flalign}


\subsubsection*{2. Translation}

For $f \in \dist(\reals^N)$ and $g \in \dist(\reals^M)$ and $h \in \reals^N$ then,

\begin{eqnarray}
    (f \otimes g) (x+h, y) = f(x+h) \otimes g(y)
\end{eqnarray}

\subsubsection*{3. Derivatives}
\begin{eqnarray}
    \partial_y^\beta\partial_x^\alpha [f(x) \otimes g(y)] = (\partial_x^\alpha f(x)) \otimes (\partial_y^\beta g(y))
\end{eqnarray}

\subsubsection*{4. Associativity}

\begin{eqnarray}
    (f \otimes g) \otimes h = f \otimes (g \otimes h)
\end{eqnarray}

\subsubsection*{5. Multiplication by $C^\infty$ function}

If $a \in C^\infty(\reals^N)$ then

\begin{flalign}
    a(x) [f(x) \otimes g(y)] = \left(a(x) f(x) \right) \otimes g(y)
\end{flalign}

\subsubsection*{6. Identity}

Consider the distribution $f(x) \otimes 1(y)$, by commutativity

\begin{flalign}
    \langle f(x) \otimes 1(y), \phi \rangle &= \langle f(x), \langle 1(y), \phi \rangle \rangle = \langle f(x), \int_{\reals^M} \phi(x, y) dy \rangle \\
    &\equiv \langle 1(y) \otimes f(x), \phi \rangle = \int_{\reals^M} \langle f(x), \phi(x, y) \rangle dy
\end{flalign}

We say that the above distribution is  `independent' of $y$, allowing one to `pick out components' of the distribution.

\section{Convolution of Distributions}

We need one more property defined on distributions before we can rigorously define the operations we used in order to solve the Poisson problem using the knowledge of the fundamental solution. This final property is a `convolution',

\begin{definition}
If $f, g \in \tfspaceD(\reals^N)$ then the convolution of $f$ and $g$ is defined as \[ (f*g) (x) = \int_{\reals^N} f(x-y) g(y) dy \] The convolution $f*g$ defines a regular distribution on $\tfspaceD(\reals^N)$ via the rule,
\[ \langle f * g, \phi \rangle = \int (f * g) (x) \phi(x) dx =  \int \phi(x) \left(\int f(x-y) g(y) dy\right) dx\]
By a version of Fubini's theorem (see below) we can swap the integral order, giving \[ \int \int \phi(x) f(x-y) g(y) dx dy\] Using a change of variables \[ \int \int f(x) g(y) \phi(x+y) dx dy = \langle f(x) \otimes g(y), \phi(x+y) \rangle \] where $\phi \in \tfspaceD(\reals^N)$
\end{definition}

\begin{theorem}[Fubini's Theorem]
    Let $F \in \tfspaceD(\underset{x}{\reals^N} \times \underset{y}{\reals^N})$.
    Then \[ \int_{\reals^{N+N}} F(x, y) dx, dy  = \int_{\reals^N} \left(\int_{\reals^N} F(x, y) dy\right) dx\] or equivalently due to linearity,
    \[ \int_{\reals^N} \left(\int_{\reals^N} F(x, y) dx\right) dy\]

\end{theorem}

Let's prove that convolution of distributions also exists, and does indeed define a valid functional. We begin in a slightly abstract manner, consider $\eta_k \in \tfspaceD(\reals^N)$ and say that it converges to 1 in $\reals^N$ if :

\begin{enumerate}
    \item $\forall K$ - compact sets $\exists$ a number $N=N(k)$ such that $\eta_k(x) = 1$ for $x \in K$, where $k \geq N$
    \item $|\partial^{\alpha}\eta_k(x)| < C_\alpha$, $x \in \reals^N$, $k = 1, 2, ...$
\end{enumerate}


Such a sequence exists, e.g. $\eta_k(x) = \eta(x/k)$, where $\eta \in \tfspaceD, \> \eta(x) = 1, \> |x| < 1$.

Let's show that the convolution of two test functions can be rewritten as,

\begin{eqnarray}
    \langle f * g, \phi \rangle = \lim_{k \rightarrow \infty} \langle f(x) \otimes g(y), \phi(x+y)\eta_k(x, y) \rangle
\end{eqnarray}

where $\phi \in \tfspaceD(\reals^N)$ and $\eta_k(x, y) \in \tfspaceD(\reals^{2N})$ such that $\eta_k \rightarrow 1$ in $\tfspaceD(\reals^{2N})$. Then the product, $\phi \eta \in \tfspaceD(\reals^{2N}) $. Now consider again the convolution definition,
we notice that $\supp f(x) g(y) \subset \bar{B_R} \subset \reals^{2N}$, therefore by the above theorem $\exists N, \> \> \forall k > N$ such that $\eta_k = 1$ in $\bar{B_R}$ such that $f(x)g(y)\eta_k(x, y) = f(x)g(y)$ for sufficiently large $k$. Therefore, we can write,

\begin{equation}
    \langle f(x) \otimes g(y), \eta_k(x, y)\phi(x+y) \rangle
\end{equation}

for our convolution of two test functions. Now we are ready to define the convolution of distributions

\begin{definition}[Convolution of Distributions]
    Suppose $f, g \in \dist(\reals^N)$ are such that $f \otimes g$ admits, \[ \langle f \otimes g, \phi(x+y) \rangle \] For test functions of the form $\phi(x+y) \in \tfspaceD(\reals^N)$, $\forall$ sequences $\{\eta_k\} \in \tfspaceD(\reals^{2N})$ such that $\eta_k \rightarrow 1$ in $\reals^{2N}$ there exists a limit \[ \lim_{k\rightarrow \infty} \langle f(x) \otimes g(y), \eta_k(x, y)\phi(x+y) \rangle \] And the limit does not depend on the specific sequence, but only on the convergence properties. Then the convolution $f*g$ is the functional,
    \[ \langle f*g, \phi \rangle =  \lim_{k\rightarrow \infty} \langle f(x) \otimes g(y), \eta_k(x, y)\phi(x+y) \rangle\ \]
\end{definition}

Let's now show that the convolution of two distributions is in fact a valid functional in $\dist(\reals^N)$ by checking the continuity of linear functionals.

Consider,

\begin{equation}
    \langle f(x) \otimes g(y), \eta_k(x, y)\phi(x+y) \rangle
\end{equation}


Let $\phi_j \overset{\tfspaceD(\reals^N)}{\rightarrow} 0$, then $\eta_k(x, y)\phi_j(x+y) \overset{\tfspaceD(\reals^{2N})}{\rightarrow} 0$ as $j \rightarrow \infty$ ($k$ is fixed). And, since $ f otimes g$ is continuous on the dual space, we see that,

\begin{eqnarray}
    \langle f \otimes g, \eta_k \phi(x+y) \rangle \rightarrow 0
\end{eqnarray}

Therefore the convolution of two distributions is continuous, and can be seen to be a distribution itself.

\subsection{Example of Convolution}

$f \in \dist(\reals^N)$, $\delta \in \dist(\reals^N)$, what is $f * \delta$?
Take a test function $\phi \in \tfspaceD(\reals^N)$, then

\begin{flalign}
    \langle f*\delta, \phi \rangle = \lim_{k \rightarrow \infty} \langle f(x) \otimes \delta(y), \phi(x+y)\eta_k(x,y) \rangle \\
    \implies \langle f(x), \langle \delta(y), \phi(x+y) \eta_k(x, y) \rangle \rangle \\
     = \langle f(x), \phi(x) \eta_k(x, 0) \rangle
\end{flalign}

We know that $\eta_k \rightarrow 0$ in $\reals^{2N}$ implies that $\exists N$ such that $\eta_k(x, 0) = 1$ on $\supp \phi$ for $k > N$,

\begin{flalign}
    = \langle f(x), \phi(x) \rangle \\
    \implies f* \delta = f
\end{flalign}

\begin{theorem}
    Let $f, g \in \dist(\reals^N)$ and $\supp g$ is bounded, then $f * g$ exists and can be written as \[ \langle f*g, \phi \rangle = \langle f(x) \otimes g(y), \eta(y) \phi(x+y) \rangle \] $\forall \phi \in \tfspaceD(\reals^N)$ where $\eta \in \tfspaceD(\reals^N)$ such that $\eta \equiv 1$ on a neighbourhood of $\supp g$
\end{theorem}

\subsection{Properties of Convolution}

\subsubsection*{1. Commutativity}

Let $f, g \in \dist$, then if $f*g$ exists, then $g*f$ exists and $f*g = g*f$. To prove this statement, consider the action of the convolution,

\begin{eqnarray}
    \langle f*g, \phi \rangle = \lim_{k\rightarrow \infty} \langle f(x) \otimes g(y), \eta_k (x, y)\phi(x+y) \rangle
\end{eqnarray}

From the commutativity of the direct product,

\begin{flalign}
    &= \lim_{k\rightarrow \infty} \langle g(y) \otimes f(x), \eta_k (x,y)\phi(x+y) \rangle \\
    &=\lim_{k\rightarrow \infty} \langle g(y) \otimes f(x), \eta_k (y, x)\phi(x+y) \rangle \\
    & = \langle g * f, \phi \rangle
\end{flalign}

we're allowed to `swap' arguments of $\eta_k$ since $\eta_k(x, y) \rightarrow 1$ implies that $\eta_k(y, x) \rightarrow 1$ in $\reals^{2N}$

\subsubsection*{2. Translation}

If $f*g$ exists then so does $f(x+h)*g(x)$ where $h \in \reals^N$ and

\begin{eqnarray}
    f(x+h)*g(x) = (f*g) (x+h)
\end{eqnarray}

\subsubsection*{3. Reflection}

If $f*g$ exists then so does $f(-x)*g(-x)$ and,

\begin{equation}
    f(-x)*g(-x) = (f*g)(-x)
\end{equation}

\subsubsection*{4. Differentiation}

If $f*g$ exists then so does $\partial^\alpha f * g$ and $f * \partial^\alpha g$ and

\begin{equation}
    (\partial^\alpha f) * g = \partial^\alpha (f * g)  = f * \partial^\alpha g
\end{equation}


\section{Fundamentality of Fundamental Solutions}

We are finally equipped with the tools to understand \textit{why} fundamental solutions are so important. Consider a linear differential operator with constant coefficients,

\begin{equation}
    L = \sum_{|\alpha| \leq m } a_\alpha \partial^\alpha
\end{equation}

where $a_\alpha \in \complexes$, and as before the fundamental solution is defined by the solution of the equation,

\begin{equation}
    LE = \delta
\end{equation}

Consider the general partial differential equation,

\begin{equation}
    L u = f
\end{equation}

where $f$ is a distribution with compact support. Then $E*f$ exists and therefore,

\begin{equation}
    L(E*f) = \sum_{|\alpha| \leq m} a_\alpha \partial^\alpha (E*f) = \left(\sum a_\alpha \partial^\alpha E \right) * f = \delta * f = f
\end{equation}

Thus we observe that if we know the fundamental solution, we can find a weak solution for arbitrary right hand sides by using a convolution. Of course there are the usual conditions on the right hand side, such as compact support and continuity.

\subsubsection*{Example}

Consider the Poisson problem in $\reals^3$

\begin{equation}
    \Delta u = f
\end{equation}

with $f \in \tfspaceD$, we know that $u = -\frac{1}{4\pi}\frac{1}{|x|} * f \in \dist$ solves the problem for $\forall f \in \tfspaceD$. We can actually go further, as we know that $\frac{1}{|x|} \in \lone$ and $f \in \dist$ therefore the convolution is also in $\lone$, and is a regular distribution. In fact we will soon see that it is also a $C^\infty$ function, and thus is not only a weak solution - but a solution in the classical sense too. Fundamental solutions offer us a way of generating general solutions to linear partial differential equations.

\section{Schwartz Kernel and the Kernel Theorem}

An integral transform is a map of the form,

\begin{equation}
    f \mapsto Kf = \int_Y k(x, y) f(y) dy, \> \> \> x \in X
\end{equation}

where $k(x, y)$ is the kernel of transformation, defined on the domain $X \times Y$. The convolution can be considered to be a form integral transform. Consider a more general integral transform. Let $k \in \dist(\underset{X}{\reals^N} \times \underset{Y}{\reals^M})$ then, $K : \tfspaceD(\reals^M) \rightarrow \dist(\reals^N)$. We define the operation,

\begin{equation}
    \langle K \psi, \phi \rangle \overset{\text{def}}{=} \langle k(x, y), \psi(y)\phi(x) \rangle
\end{equation}

For $\forall \phi \in \tfspaceD(\reals^N), \> \forall \psi \in \tfspaceD(\reals^M)$. The functional is clearly linear on $\tfspaceD(\reals^N)$ and it is also seen to be continuous. Therefore $K\psi \in \dist(\reals^N)$.

\vspace{5pt}

We can check the continuity of the kernel operator, $K$, in mapping from $\tfspaceD(\reals^M) \mapsto \dist(\reals^N)$. Take $\psi_n \rightarrow 0$ in $\tfspace(\reals^M)$, then we have,

\begin{equation}
    \langle K \psi_n, \phi \rangle = \langle k, \psi_n\phi \rangle \rightarrow 0
\end{equation}

since $\psi_n(y)\phi(x) \rightarrow 0$ in $\tfspace(\reals^{N+M})$, that is $K\psi_n \rightarrow 0$ in $\dist(\reals^N)$ therefore $K$ is continuous.

\begin{theorem}[Schwartz Kernel Theorem]
    A linear map $A : \tfspaceD{\reals^M} \mapsto \dist(\reals^N)$ is sequentially continuous if and only if it is generated by a kernel $a \in \dist(\reals^{N+M})$. This is called a Schwartz kernel, \[ \langle A \psi, \phi \rangle = \langle \, \phi \otimes \psi \rangle \] For all $\phi \in \tfspaceD(\reals^N)$ and $\psi \in \tfspaceD(\reals^M)$
\end{theorem}

Intuitively, this theorem states that any `reasonable' linear map has a Schwartz kernel.

\section{The Fourier Transform}

Fourier transforms will be a vital technique for solving linear partial differential equations. Furthermore, their exists numerous well known software libraries for numerically performing Fourier transforms - which make them an invaluable ally in tackling PDEs.

\vspace{5pt}

Let's start with the definition, the Fourier transform of a function $f$ on $\reals^N$ is,

\begin{flalign}
    \hat{f}(\zeta) = \int_{\reals^N} e^{-i x \cdot \zeta} f(x) dx = \mathcal{F}[f](\zeta) = \mathcal{F}_{x\rightarrow\zeta}[f(x)]
    \label{eq:ft}
\end{flalign}

where $\zeta \in \reals^N$. This operation is always well defined if $f \in \lone(\reals^N)$.

\section{Properties of Fourier Transform}

\subsubsection*{1. Continuity of Transform}

Let $f \in \tfspaceD$ then $\hat{f} \in C$. To prove this statement we need to show that $\hat{f}(\zeta_u) \rightarrow \hat{f}(\zeta)$ if a sequence converges $\zeta_u \rightarrow \zeta$, we know that $f \in \tfspaceD$ so $\supp f \subset B_R$

\begin{flalign}
    \hat{f}(\zeta_u) &= \int_{B_R} e^{-i x \cdot \zeta_n}f(x) dx \\
    & = \int_{B_R} \left( e^{-i x \cdot \zeta_n} - e^{-ix \cdot \zeta} \right) f(x) dx + \int_{B_R} e^{-ix \cdot \zeta}f(x) dx = \hat{f}(\zeta)
\end{flalign}

The first integral term converges to zero due to the convergence of the sequence $\zeta_n$, and we recognise the second term as being a Fourier transform. Thus the identity follows.

\subsubsection*{2. Convolution}

Let $f, g \in \tfspaceD$ then if $f*g \in \lone$ we find $\mathcal{F}(f*g) = \hat{f} \cdot \hat{g}$ where the multuplication is done in the usual sense, rather than in the sense of distributions. To prove this, we know that if $f*g \in \lone$ then $\widehat{f*g}$ is well defined, therefore

\begin{flalign}
    \widehat{f*g}(\zeta) &= \int_{\reals^N}e^{-ixz}(f*g)(x) dx \\
    &= \int_{\reals^N} e^{-ix \cdot \zeta} \left( \int_{\reals^N} f(x-y) g(y) dy \right) dx
\end{flalign}

Applying Fubini's theorem, and a change of variables $x = y+z$,

\begin{flalign}
    &= \int \int f(z) g(y) e^{-i(y+z)\zeta} dzdy \\
    &= \hat{f}(\zeta)\hat{g}(\zeta)
\end{flalign}

\subsubsection*{3. Associativity}
Let $f, g \in \tfspaceD$ then

\begin{equation}
    \int_{\reals^N} f(x) \hat{g}(x) dx = \int_{\reals^N} \hat{f}(\zeta) g(\zeta) d\zeta
\end{equation}

This means that,

\begin{flalign}
    \int \hat{f} g dx = \int f \hat{g} dx \\
    \int \hat{f} \phi dx = \int f \hat{\phi} dx \\
    \langle \hat{f}, \phi \rangle = \langle f, \hat{\phi} \rangle
\end{flalign}

The proof for this final property, can be seen from a direct calculation of the left hand side.

\section{Schwartz Space}

\section{Fourier Transform in Schwartz Space}

\section{Fourier Inversion Theorem}

\section{Tempered Distributions}

\section{Fourier Transform on Tempered Distributions}

\section{Invertibility of FT on $S'$, \& Operational Rules}


\section{Fourier Transform of Distributions with Compact Support}

\end{document}
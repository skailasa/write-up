\documentclass[12pt, a4]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\usepackage[backend=bibtex]{biblatex}
\addbibresource{dist.bib}

\title{Linear Partial Differential Equations}
\author{Srinath Kailasa \thanks{srinath.kailasa.18@ucl.ac.uk} \\ \small University College London}

\date{\today}



\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{notation}{Notation}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{problem}[theorem]{Problem}

\DeclareMathOperator\supp{supp}
\DeclareMathOperator\reals{\mathbb{R}}
\DeclareMathOperator\complexes{\mathbb{C}}
\DeclareMathOperator\tfspace{C_0^\infty}
\DeclareMathOperator\tfspaceD{\mathcal{D}}
\DeclareMathOperator\dist{\mathcal{D'}}
\DeclareMathOperator\lone{L_{\text{loc}}^1}
\DeclareMathOperator\singsupp{sing supp}

\newcommand{\twopartdef}[4]
{
	\left\{
		\begin{array}{ll}
			#1 & \mbox{if } #2 \\
			#3 & \mbox{if } #4
		\end{array}
	\right.
}

\begin{document}

\maketitle

\section*{Abstract}

In this document I summarise some of the main applications of the theory of distributions for the solution of partial differential equations via the method of `fundamental solutions'. I begin by introducing the key concepts behind the idea of the method of distributions, with the goal of solving the Laplace and Heat equations using these ideas. We proceed to an overview of the Fourier transform, and its usage for solving PDEs, and its description within the theory of distributions. We conclude with a soujourn on Sobolev spaces, and a short description of their utility in extending weak solutions, derived using distribution theory, to full classical solutions of PDEs. Furthermore, this document also includes a `scratch pad' section, in which I provide further examples and proofs of the applications of these ideas.

\section{Distributions and Test Functions}

\subsection{Motivation}

Consider the wave equation in 1D,

\begin{equation}
    \frac{\partial^2u(x, t)}{\partial t^2} = \frac{\partial^2u(x, t)}{\partial x^2}
    \label{eq:wave_eq_1d}
\end{equation}

Any twice continuosly differentiable function $f \in C^2(\reals)$ of the form $u(x, t) = f(x-t)$ satisfies the wave equation. However, we face difficulties if the solution is admissable in a physical sense, but has mathematical problems such as being non-continuous, e.g. $u(x, t) = |x-t|$. This is the motivation behind seeking a new theory, a theory of distributions, which allow us to generalise notions of derivatives and solve problems with solutions that may not not be mathematicallly particlarly `nice' as they may be required by the physics of the problem. These generalisations are known as distributions.

\subsection{Defining Test Functions}

A few basic definitions will serve us well for the remainder of these notes.

\begin{definition}[Classes of Continuous Functions]
Let $\Omega^N$ be an open subset and let $m$ be a non-negative integer. The class $C^m(\Omega)$ consists of functions on $\Omega$ which have continuous derivatives of order less than or equal to $m$. Furthermore (1) $C^0(\Omega) = C(\Omega)$  is simply the class of all continuous functions on $\Omega$ and (2) $C^\infty (\Omega)$ is the class of functions with derivatives of all orders.
\label{def:c_m_functions}
\end{definition}

\begin{definition}[Support of Functions]
The support of a function $f : \Omega \rightarrow \mathbb{C}$ is the closure of the set $\{x \in \Omega | f(x) \neq 0\}$
\[ \supp f = \overline{\{x \in \Omega | f(x) \neq 0\}}\]
\label{def:support_of_functions}
\end{definition}

Examples in $\Omega = \reals$

\begin{itemize}
    \item $f(x) = 0$, then  $\supp f = \emptyset$
    \item $f(x) = x$, then $\supp f = \reals$
\end{itemize}

\begin{definition}
    $C_0^\infty(\reals^N)$ is the subset of $C^\infty(\reals^N)$ consisting of functions with \textbf{compact} support.
\end{definition}

\begin{definition}[Compact]
    Compact in this context means a \textbf{closed} and \textbf{bounded} set.
    \[ \Omega \subset \reals^N\]
    is bounded if $\exists R > 0$ such that
    \[\Omega \subset B_R = \{ x : |x| < R\}\]
    and $|x|$ can be understood as a length in $\reals^N$.
    \label{def:compact_function}
\end{definition}

Now we list some of the basic properties of the function space described by $\tfspace$, as this is the space from which we seek our coveted test functions. We can think of it as a linear vector space, with the usual properties. That is, for $\phi_1, \phi_2 \in \tfspace$ and $\alpha_1, \alpha_2 \in \complexes$,

\begin{eqnarray}
    \alpha_1\phi_1 + \alpha_2\phi_2  \in \tfspace
\end{eqnarray}

Before arriving at the final definition of test functions, we must also introduce the concept of the multi-index. This has a familiar whiff about if for those who are familiar with vector-calculus.

\begin{notation}[Multi-Index]
    A multi-index is a vector with $N$ components, \[ \alpha=(\alpha_1,...,\alpha_N) \] where each component is a non-negative integer. It has an \textbf{order}, described by the sum of all components $|\alpha| = \alpha_1+...+\alpha_N$. If $\beta$ is also a multi-index, then $\alpha+\beta$ is a component-wise sum.
\end{notation}

Using multi-indices we can denote the derivative of a function $f(x_1,x_2,...)$ with respect to a multi-index as,

\begin{eqnarray}
    \partial^\alpha f = \frac{\partial^{|\alpha|}}{\partial x_1^{\alpha_1}, \partial x_2^{\alpha_2}, ..., \partial x_N^{\alpha_N}}
\end{eqnarray}

Furthermore, for $\phi \in \tfspace$ any $\partial^\alpha \phi \in \tfspace$. If $a \in C^\infty$ i.e. a doesn't have compact support, $a\phi \in \tfspace$ - its product with a function with compact support will also have compact support. Finally changes of variables, will also leave us within $\tfspace$ i.e. for an arbitrary vector $b \in \reals^N$ and an $N \times N$ matrix $A$, $\phi(Ax+b) \in \tfspace$.

Now we have a promising vector space, with useful properties for it's members such as compactness and boundedness. There is however one more thing we need to consider, which is the idea of convergence within the vector space.

\begin{definition}
    A sequence $\{\phi_i\}^\infty_{i=1} \in \tfspace $ is said to converge to zero if
    \begin{enumerate}
        \item There exists a compact set $K \subset \reals^N$ such that $\supp \phi_j \subset K$ for all $i=1,2,....$
        \item For each multi-index $\alpha$ the derivative $\partial^\alpha \phi_j$ converges to zero uniformly as $j \rightarrow \infty$. That is, for all $\alpha$, $\underset{x \in \reals^N}{\supp} |\partial^\alpha \phi_j(x)|\rightarrow 0$
    \end{enumerate}

    \label{def:convergence_of_tf}
\end{definition}


The set $\tfspace$ with the above convergence properties is called the \textbf{space of test functions}, and denoted by $\tfspaceD$. We remark that the convergence property is often denoted with respect to $\tfspaceD$ in the literature as,

\begin{eqnarray}
    \phi_N \overset{\tfspaceD}{\rightarrow} \phi
\end{eqnarray}

which further implies,

\begin{eqnarray}
    \phi_N - \phi \overset{\tfspaceD}{\rightarrow} 0
\end{eqnarray}

\subsection{Defining Distributions}

Consider two abstract sets $A$ and $B$, the map between them $f : A \rightarrow B$ defines a \textbf{functional}. The map $f$ is considered to be continuous if $a_n \overset{A}{\rightarrow} a$ implies that $f(a_n) \overset{B}{\rightarrow} f(a)$. Strictly, this is `sequential continuity'.

\begin{definition}
    The set of distributions, $\dist$, is the set of all \textbf{linear} and \textbf{ (sequentially) continuous} functionals that map $f : \tfspaceD \rightarrow \complexes$
\label{def:distribution}
\end{definition}

We see that the distributions are defined by mapping from the space of test functions to (potentially complex) numbers. This concept will allow us to generalise notions of differentiability.

If $f$ is a functional, then

\begin{enumerate}
    \item For each $\phi \in \tfspaceD$, the functional associates a number denoted by the following Bra-Ket notation $\langle f, \phi \rangle \in \complexes$. This is also described as the \textbf{action} of the functional.
    \item The functional is linear such that for $\alpha_1, \alpha_2 \in \complexes$ and $\phi_1, \phi_2 \in \tfspaceD$, $\langle f, \alpha_1\phi_1, \alpha_2\phi_2 \rangle = \alpha_1\langle f, \phi_1 \rangle + \alpha_2\langle f, \phi_2 \rangle $
    \item A distribution $f$ is called (sequntially) continuous on $\tfspaceD$ if $\langle f, \phi_k \rangle \rightarrow \langle f, \phi \rangle$ where $\phi_k \overset{\tfspaceD}{\rightarrow} \phi$ as $k \rightarrow \infty$
\end{enumerate}

Let us illustrate the concept of functionals which satisfy the definition of distributions by going through some examples.

\begin{enumerate}
    \item \textbf{Continuous functions as distributions}. Let $f \in C(\reals^N)$ and define a functional with the mapping $\tfspaceD(\reals^N) \rightarrow \complexes$ as $\langle f, \phi \rangle = \int_{\reals^N} f(x) \phi(x) dx < +\infty$ which is valid for $\forall \phi \in \tfspaceD$. The integral is linear, by inspection, so it remains to show that it is also continuous in the sense of sequential continuity. Consider a sequence of test functions $\phi_n \in \tfspaceD$ such that $\phi_n \rightarrow \phi$ in $\tfspaceD$, without loss of generality let us assume that $\phi = 0$ so we have to show now that the action $\langle f, \phi_n \rangle \rightarrow 0$. From the definition of convergence in $\tfspace$ (\ref{def:convergence_of_tf}), we know that there exists a compact set $K$ for which $\supp \phi_n \subset K \> \> (\forall n)$. Then, \[ |\langle f, \phi_n \rangle | = \left |\int_{\reals^N}f \phi_n dx \right | = \left |\int_K f \phi_n dx \right | \leq \sup_{x \in \reals^N} |\phi_n| \int_K |f| dx\]. From (\ref{def:convergence_of_tf}) we see that $\sup_{x \in \reals^N} |\phi_n| \rightarrow 0$ and the integral over $|f|$ in $K$ is finite due to the compactness of $K$, therefore $\langle f, \phi_n \rangle \rightarrow 0$ and the functional is sequentiallly continuous. A remark about notation, we've identified above a function $f$ with a \textit{functional} which we also denote by $f \in \dist$.
    \item \textbf{Heaviside function}. It therefore follows from above that a function defines a distribution if the following statement holds, \[ \int_K |f| dx < + \infty, \> \> \forall K \subset \reals^N \]

    Consider the Heaviside function in one dimension, \[ H(x) = \begin{cases}
        1, x > 0 \\
        0, x \leq 0
    \end{cases}\]
    Then we can define the action of the distribution generated by the Heaviside function as, \[ \langle H, \phi \rangle = \int_{-\infty}^{\infty} H(x) \phi(x) dx = \int_{0}^{\infty} \phi(x) dx, \> \> \forall \phi \in \tfspaceD \]

    Therefore we see that it also identifiable as a distribution.
\end{enumerate}

Before proceeding, we need the concept of regular distributions,

\begin{definition}[Regular Distribution]
A distribution $f \in \dist$ is called regular if $\exists f \in \lone$ such that \[ \langle f, \phi \rangle = \int_{\reals^N} f(x) \phi(x) dx, \> \> \forall \phi \in \tfspaceD\]
    \label{def:regular_dist}
\end{definition}

Here $\lone$ defines a function space where each member is `locally integrable', i.e. has a finite integral on every compact subset of its domain of definition. More formally, let $\Omega$ be an open set in $\reals^N$ and $f : \Omega \rightarrow \complexes$ if the following integral holds,
\begin{equation}
    \int_K |f| dx < + \infty
    \label{eq:l_one_loc}
\end{equation}

for all subsets $K \subset \Omega$, then $f \in \lone$. The 1 refers to the power of the absolute value in the integral. We then notice that the conditions for sequential continuity are satisfied if a functional is chosen from $\lone$.

\vspace{5pt}

As a final example of this section, consider the Dirac $\delta$-function, defined by the following action,

\begin{eqnarray}
\langle \delta, \phi \rangle = \phi(0), \> \> \forall \phi \in \tfspaceD(\reals^N)
    \label{eq:dirac}
\end{eqnarray}

The functional is clearly in $\lone$, thefore satisfies sequential continuity, with linearity obvious by inspection. Therefore we see that it satisfies the properties required of a distribution.

\subsection{Basic Operations on Distributions}

Here, we state some basic facts about operations on distributions largely without proof.

\subsubsection{Multiplication by a $C^\infty$ function}
For example, If $a \in C^\infty(\reals^N)$ and $f \in C(\reals^N)$ then $f \in \dist$ then,

\begin{equation}
    \langle af, \phi\rangle = \int_{\reals^N}(a(x)f(x))\phi(x)dx = \int_{\reals^N}f(x) (a(x)\phi(x))dx = \langle f, a\phi \rangle
\end{equation}

Implying that $a\phi \in \tfspace$. This identity can be extended to all distributions.

\begin{definition}
    If $a \in C^\infty$ and $f \in \dist$ then, $af \in \dist$ defined by \[
        \langle af, \phi \rangle = \langle f, a\phi \rangle, \>\> \forall \phi \in \tfspaceD\]
\end{definition}

One can check that the above does define a distribution through an example. Consider $f = \delta \in \dist$ and $a \in \tfspace$, then

\begin{eqnarray}
    \langle a\delta, \phi \rangle = \langle \delta, a \phi \rangle = a(0)\phi(0) = a(0) \langle \delta, \phi \rangle \> \> \forall \phi \in \tfspaceD
\end{eqnarray}

This implies that $a\delta = a(0) \delta$ and does define a distribution.

\subsubsection{Differentiation}

Differentiation for distributions uses a form of Green's identity. Consider the one dimensional case, $f \in C(\reals)$ and $\phi \in \tfspaceD(\reals)$ and has compact support, then

\begin{equation}
    \int_{-\infty}^{\infty} f'(x) \phi(x) dx = \int_a^b f'(x)\phi(x)dx = \left [ f \phi \right ]_a^b - \int_a^b f\phi' dx = -\int_a^b f\phi' dx
\end{equation}

The boundary term vanishes due to the compact support of $\phi$. Generalising the above identity for higher order derivatives, we see that,

\begin{equation}
    \int_{-\infty}^{\infty} \frac{d^m}{dx^m} f(x) \phi(x) dx = (-1)^m \int_{-infty}^{\infty} f(x) \frac{d^m}{dx^m}\phi(x) dx
\end{equation}

In the language of actions this goes to,

\begin{eqnarray}
    \langle \frac{d^m}{dx^m}f, \phi \rangle = \langle f, \frac{d^m}{dx^m} \phi \rangle
\end{eqnarray}

Furthermore, we state that this differentiated object defines a distribution too.

\begin{definition}
    Let $f \in \dist(\reals^N)$. Then for any multi-index $\alpha$ the distribution $\partial^\alpha f$ is defined to be, \[ \langle \partial^\alpha f, \phi \rangle = (-1)^{|\alpha|} \langle f, \partial^\alpha \phi \rangle, \> \> \forall \phi \in \tfspaceD(\reals^N) \]
    \label{def:differentiate_distribution}
\end{definition}

Let's consider some simple corollary examples,

\begin{enumerate}
    \item Consider a one dimensional problem in $\reals$ where $\delta \in \dist$. What is $\frac{d}{dx}\delta$? From the above definition, \[
        \langle \frac{d}{dx} \delta, \phi\rangle = - \langle \delta, \frac{d}{dx}\phi \rangle = -\phi'(0)\]
    \item Consider again a one dimension problem with the Heaviside function $H \in \dist$. What is $\frac{d}{dx}H$? Using the same procedure, \[
        \langle \frac{d}{dx}H, \phi \rangle = -\langle H, \frac{d}{dx} \phi \rangle = \int_0^\infty \frac{d\phi}{dx}dx = -[\phi]_0^\infty = \phi(0) = \langle \delta, \phi \rangle, \> \> \forall \phi \in \tfspaceD \]. This implies that $\frac{dH}{dx} = \delta$, in a distributional sense.
\end{enumerate}

\subsubsection{Change of Variables ($x = Ay + b$)}

Finally let's consider a linear transformation, corresponding to a change of variables. Here $A$ is a non-singular $N \times N$ matrix and $b \in \reals^N$. If $f \in C^\infty(\reals^N)$ and $\phi \in \tfspaceD(\reals^N)$, then

\begin{equation}
    \int_{\reals^N}f(Ay+b)\phi(y)dy = \int_{\reals^N}f(x)\phi(A^{-1}(x-b))\frac{1}{\text{det}(A)}dx
\end{equation}

This also defines a distribution, giving us our final definition in this section,

\begin{definition}
    Let $f \in \dist(\reals^N)$, with $A$ and $b$ as above, then $f(Ay+b) \in \dist$ is defined by \[\langle f(Ay+b), \phi(y)\rangle = \langle f(x), \phi(A^{-1}(x-b))\frac{1}{|A|}\rangle, \> \> \forall \phi \in \tfspaceD(\reals^N)\]
\end{definition}

\subsection{The Support of Distributions}

Distributions do not take `values' in a domain like a function would, but we define an equivalent statement by using their action. For example for a distribution $f$, the statement $f=0$ is read to mean that $\langle f, \phi \rangle = 0, \> \> \forall \phi \in \Omega, \> \> s.t. \> \> \supp \phi \subset \Omega$.

\begin{remark}
    For $f, g \in \dist$, then $f=g$ in $\Omega$ means that, \[ \langle f, \phi \rangle = \langle g, \phi \rangle, \> \> \forall \phi \in \tfspaceD(\Omega)\]
\end{remark}

We can now use this technology to define the support of a distribution.

\begin{definition}
    Let $f \in \dist(\reals^N)$. Then $\supp f$ is the complement of the set \[ O_f = \left \{ x \in \reals^n : f = 0 \> \> \text{on a neighbourhood of x}  \right \} \] This $O_f$ is known as the zero-set of $f$.
\end{definition}

\subsection{The Convergence of Distributions}

Recalling our sequential continuity condition, let $\{f_j\}_{j=1}^\infty$ be a sequence of distributions on $\reals^N$ where $f \in \dist(\reals^N)$.

\begin{definition}
    We say that $f_n$ converges to $f$ in $\dist$, and write $f_n \overset{\dist}{\rightarrow} f$, if \[ \langle f_n, \phi \rangle \rightarrow \langle f, \phi \rangle, \> \> \forall \phi \in \tfspaceD \] as $n \rightarrow \infty$
\end{definition}

Consider $f_n(x) = \frac{1}{n} \sin(nx)$. Clearly $f_n \in \lone(\reals)$, therefore $f_n$ defines a distribution, with the action

\begin{equation}
    \langle f_n, \phi \rangle = \int_{\reals} f_n(x) \phi(x) dx
\end{equation}

Indeed it's obvious that $f_n \rightarrow 0$ in the sense of $\dist$, noticing that

\begin{equation}
    | \langle f_n, \phi \rangle | = \left | \int_{\reals} \frac{1}{n} \sin(nx) \phi(x) dx\right | \leq \frac{1}{n} \left | \int_{\reals}\phi(x) \right |
\end{equation}

This leads to the following proposition. If $f_n \overset{\dist}{\rightarrow} f$, then $\partial^\alpha f_n \overset{\dist}{\rightarrow} \partial^\alpha f$ for all multi-indices $\alpha$. Consider a corollary example, where $g_n(x) = \cos(nx)$, then $g_n \overset{\dist}{\rightarrow} 0 $ as $\frac{d}{dx}(\frac{1}{n}\sin(nx)) = g_n$ and therefore we can apply the above result.

\vspace{5pt}

For a more involved example, consider $f \in \lone(\reals)$ and let $\supp f \in [-1, 1]$ with $\int_{-1}^1 f(x) dx = 1$ and consider the sequence defined by $f_k(x) = kf(kx)$ where $k=1,2,3...$ . Let us show that $f_k \overset{\dist}{\rightarrow} \dist$. Equivalently we can show that there is no difference between the actions of both functionals.

\begin{flalign}
    &\left | \langle f_k, \phi \rangle - \langle \delta, \phi \rangle \right | \\
    &= \left | \int_{-\infty}^\infty f_k(x)\phi(x)dx - \phi(0) \right | \\
    &= \left | \int_{-\infty}^\infty kf(kx)\phi(x)dx - \phi(0)\int_{-\infty}^\infty f(x)dx \right | \\
    &= \left | \int_{-1}^{1}f(x) ( \phi(\frac{x}{k}) - \phi(0) ) dx \right| \\
    &\leq \max_{x \in [-1, 1]} | \phi(\frac{x}{k}) - \phi(0)| = \max_{t \in [-1/k, 1/k]} | \phi(t) - \phi(0)| \rightarrow 0
\end{flalign}

These examples lead us to the final theorem of this section,

\begin{theorem}[Completeness of $\dist$]
    Consider the sequence of distributions $f_1,...,f_n,...$ such that for all $\phi \in \tfspaceD$ the sequence $\langle f_k, \phi \rangle$ converges as $k \rightarrow \infty$. Then the functional defined by, \[ \langle f, \phi \rangle = \lim_{k \rightarrow \infty} \langle f_k, \phi \rangle \] is linear and continuous, i.e it defines a distribution $f \in \dist$.
\end{theorem}

The intuitive interpretation of this theorem is basically that we can't leave the space of distributions by summing distributions.

\subsection{Equivalent Definition of Distributions}

\begin{notation}
    If $\Omega$ is an open set in $\reals^N$ and $K$ is a compact subset of $\Omega$ and $u \in C^m(\Omega)$ then \[ \| u \|_{C^m(K)} = \max_{x \in K, \> |\alpha| \leq m} |\partial^\alpha u| \]
\end{notation}

This allows us to write down our equivalent formulation of distributions.

\begin{definition}
    A linear functional $f$ on $\tfspaceD(\reals^N)$ is a distribution iff for all compact sets $K$ in $\reals^N$ there exist $C$ and $m$ such that,
    \[ |\langle f, \phi \rangle| \leq C \| \phi \|_{C^m(k)}\] For all $\phi \in \tfspaceD$ such that $\supp \phi \subset K$.
\end{definition}

Proving the reverse implication. We have the above definition, and we need to show that it satisfies sequential continuity. That is if $\phi_n \overset{\tfspaceD}{\rightarrow} \phi$, then $\langle f, \phi_n \rangle \overset{\dist}{\rightarrow} 0$

\vspace{5pt}

Proving the forward implication. Assume that $f \in \dist$, then let's prove by contradiction. Assume the alternative definition doesn't hold,

\begin{eqnarray}
    | \langle f, \phi_m \rangle | > m \| \phi_m \|_{C^m(K)}
\end{eqnarray}

Define $\psi_m = \frac{\phi_m}{m \| \phi_m \|_{C^m(K)}}$. Then $\psi_m \in \tfspaceD$, $\supp \psi_m \subset K$ and for any multi-index $\beta$ such that $m \geq |\beta|$, we have,

\begin{eqnarray}
    \| \psi_m \|_{C^{|\beta|}(K)} = \frac{\|\phi_m\|_{C^{|B|}(K)}}{m \|\phi_m\|_{C^m(K)}} \leq \frac{1}{m}
\end{eqnarray}

Consequently $\psi_m \overset{\tfspaceD}{\rightarrow} 0$. However, this contradicts our initial assumption.

\section{Fundamental Solution of Laplacian in $\reals^3$}

For a linear partial differential equation with linear operator $L$,

\begin{eqnarray}
    LF = \delta(x)
\end{eqnarray}

the fundamental solution, $F$,  is defined as that which yields the $\delta$-function up to a constant, and is therefore defined as the solution of the above equation.

Consider the Poisson problem,

\begin{eqnarray}
    \Delta u = f
    \label{eq:laplace}
\end{eqnarray}

Where the linear partial differential operator, the Laplace operator, is given by

\begin{eqnarray}
    \Delta = \sum_{i=1}^N \frac{\partial^2}{\partial x_i^2}
\end{eqnarray}

Let's check that the fundamental solution in $\reals^3$ is described by the following relation,

\begin{equation}
    \Delta \frac{1}{|x|} = -4\pi \delta(x)
\end{equation}

where as usual $|x| = \sqrt{x_1^2+x_2^2+x_3^2}$. We can check that the Laplacian of the fundamental solution is 0 in $\reals^3 \backslash \{ 0\}$ using our ordinary notions of derivative,

\begin{eqnarray}
    \frac{\partial}{\partial x_1} \frac{1}{|x|} = -\frac{x_1}{(x_1^2+x_2^2+x_3^2)^{\frac{3}{2}}}
\end{eqnarray}

and,

\begin{eqnarray}
    \frac{\partial^2}{\partial x_1 ^2} \frac{1}{|x|} = \frac{3x_1^2}{(x_1^2+x_2^2+x_3^2)^{\frac{5}{2}}}-\frac{1}{(x_1^2+x_2^2+x_3^2)^{\frac{3}{2}}}
\end{eqnarray}

from which the result follows. It's not really possible to define derivatives at the origin for our fundamental soltuion in the usual sense, however we can show that $1/|x| \in \lone$ - this would allow us to define it as a distribution with all of the useful properties that they have. To see that the statement is true, we simply have to observe that the integral,

\begin{eqnarray}
    \int_{K} \frac{1}{|x|} dx < +\infty
\end{eqnarray}

for all compact subsets $K \subset \reals^3$. If this is the case, then let's take a ball such that $K \subset B_R = \{ x | x < R\}$. Then,

\begin{flalign}
    \int_K |f| dx \leq \int_{B_R} |f| dx \leq \int_0^R \int_{S_r} |f| d\omega dr
\end{flalign}

where we've switched to using spherical coordinates defined by the surface of a sphere $\omega \in S_r = \{\ x: \sqrt{x_1^2 + x_2^2 + x_3^2}\}$, and the radial coordinate $r \in [0, R]$

\begin{flalign}
    &\int_0^R \frac{1}{r} \int_{S_r} d\omega dr \\
    &\int_0^R \frac{1}{r} r^2 dr = \int_0^R r dr < +\infty
\end{flalign}

This result extends to $\reals^N$ too, and allows to say that $1/|x| \in \lone$ and therefore that $1/|x| \in \dist$ and is a regular distribution.
Therefore, let's now take derivaitives of the fundamental solution in a distributional sense,

\begin{equation}
\langle \Delta \frac{1}{|x|}, \phi \rangle = \langle \frac{1}{|x|}, \Delta \phi \rangle = \int_{\reals^3} \frac{1}{|x|} \Delta \phi dx
\end{equation}

This is of course for any $\phi \in \tfspaceD(\reals^3)$. We note that the test functions have a finite support, $\supp \phi \subset B_R$, where we've drawn a ball around the compact subset $K$ in which they are supported to help our integral evaluation,

\begin{flalign}
 \int_{B_R} \frac{1}{|x|} \Delta \phi dx = \lim_{\epsilon \rightarrow 0} \int_{B_R \backslash \bar{B_\epsilon}} \frac{1}{|x|} \Delta \phi dx
\end{flalign}

Here we're taking the integral over the ball, and excluding an ever decreasing ball that is centered over the origin of radius $\epsilon$. Let's now integrate using Green's second identity,

\begin{flalign}
    \lim_{\epsilon \rightarrow 0} \left( \int_{B_R \backslash \bar{B}_\epsilon} \Delta \frac{1}{|x|} \phi dx + \oint_{S_\epsilon} \partial_n \phi \frac{1}{|x|} ds  - \oint_{S_\epsilon} \phi \partial_n \frac{1}{|x|} ds \right)
\end{flalign}

Here we've skipped some steps in the surface integrals, specifically we note that they will be zero over the outer surface due to the compact support of the test functions, and therefore exclude them from the integral. Furthermore, the first integral is zero by definition as we've shown that the Laplacian of the fundamental solution is 0 when excluding the origin.

\vspace{5pt}

We are then left with the two surface integrals,

\begin{flalign}
    \lim_{\epsilon \rightarrow 0} \left( \oint_{S_\epsilon} \partial_n \phi \frac{1}{|x|} ds  - \oint_{S_\epsilon} \phi \partial_n \frac{1}{|x|} ds \right)
\end{flalign}

Consider the first one of these,

\begin{flalign}
    \left | \oint_{S_\epsilon} \partial_n \phi \frac{1}{|x|} ds \right | &\leq \oint_{S_\epsilon} \max_{S_\epsilon}  | \nabla \phi| \frac{1}{|x|} ds \\
    & \leq \| \phi \|_{C^1(\reals^3)} \oint_{S_\epsilon} \frac{1}{|x|} ds \\
    & = \| \phi \|_{C^1(\reals^3)} \frac{1}{\epsilon} \oint_{S_\epsilon} ds = \| \phi \|_{C^1(\reals^3)} 4\pi \epsilon \overset{\epsilon \rightarrow 0}{\rightarrow} 0
\end{flalign}

Now the second, using the fact that the unit normal vector over the inner surface (pointing towards the origin) is $\hat{n}= -\frac{x}{|x|}$ and therefore that $\partial_n = -\partial_r$

\begin{flalign}
    &\oint_{S_\epsilon} \phi \partial_r \frac{1}{r} ds = - \oint_{S_\epsilon} \phi \frac{1}{r^2}ds = -\frac{1}{\epsilon^2} \oint_{S_\epsilon} \phi(x)ds \\
    & = -\frac{1}{\epsilon^2} \oint_{S_\epsilon} \phi(x) - \phi(0) ds - \frac{1}{\epsilon^2} \oint_{S_\epsilon} \phi(0) ds \\
    & = -\frac{1}{\epsilon^2} \oint_{S_\epsilon} \phi(x) - \phi(0) ds - 4\pi \phi(0)
\end{flalign}

All that remains is to show that the first integral converges to 0 and we'll have our result,

\begin{flalign}
    &\left | \frac{1}{\epsilon^2} \int_{S_\epsilon} \phi(x) - \phi(0) ds \right | \\
    & \leq \frac{1}{\epsilon^2} \int_{S_\epsilon} \left | \phi(x) - \phi(0)\right | ds \leq 4\pi \max_{x \in S_\epsilon} | \phi(x) - \phi(0) | \rightarrow 0
\end{flalign}

Therefore we see,

\begin{flalign}
    \langle \Delta \frac{1}{|x|}, \phi \rangle = -4\pi \phi(0) = -4\pi \langle\delta, \phi \rangle
\end{flalign}

for all $\phi \in \tfspaceD$, and that $\Delta \frac{1}{|x|} = -4\pi \delta$. Note that the fundamental solution is a family of solutions, and is not unique up to a constant.

The name `fundamental solution' seems very important, and as we shall see we will be able to write general solutions of PDEs using knowledge of them. However, in order to get to that point we need to have a few more operations on distributions up our sleeves.

\section{Direct Product of Distributions}

Let's take a series of steps which are as of yet poorly defined, but motivate the need for more operations on distributions. Consider again the Poisson problem in $\reals^3$, we now know the fundamental solution for this problem,

\begin{flalign}
E(x) = -\frac{1}{4\pi}\frac{1}{|x|}
\end{flalign}

Let's displace the solution by some fixed vector $y$, the Poisson problem then goes to,

\begin{flalign}
    \Delta_xE(x-y) = \delta(x-y)
\end{flalign}

Now, for our first poorly defined operation, multiplication by a distribution,

\begin{flalign}
    f(y)\Delta_xE(x-y) = f(y)\delta(x-y)
\end{flalign}

And then our second poorly defined operation, integration over the domain with respect to this fixed vector $y$

\begin{flalign}
    \int f(y)\Delta_xE(x-y) dy = \int f(y)\delta(x-y) dy
\end{flalign}

By linearity, and using the properties of the $\delta$-function,

\begin{flalign}
    \Delta_x \int f(y)E(x-y) dy = f(x)
\end{flalign}

We see that our Poisson problem is `solved' by,

\begin{flalign}
    u(x) = \int f(y)E(x-y) dy
\end{flalign}

meaning that we can derive a weak solution of the Poisson problem with knowledge of just the fundamental solution - a hint as to why these objects are so fundamental, they are in some way atomistically tied to a more general solution of the differential equation. As we shall see they can do even better than define weak solutions. However, in order to rigorously use this intuition, we need to first learn how distributions can be in a sense `multiplied' by each other. This motivates the desire to define a `direct product' between distributions, which generalise notions of multiplications to distributions.

Let us start with defining direct products with respect to test functions in $\reals^N$. Consider $f,g \in \tfspaceD(\reals^N)$ where the product $fg \in \tfspaceD(\reals^N)$. We define the tensor or direct product as $f \otimes g \in \tfspaceD(\reals^{N+N})$ in some `dual' space. As we can describe continuous functions as distributions, it is possible to write,

\begin{flalign}
    \langle f(x)g(y), \phi(x, y) \rangle &= \int_{\reals^{N+N}} f(x)g(y)\phi(x,y)dxdy \\
    &= \int_{\reals^N} f(x) \left( \int_{\reals^N} g(y) \phi(x, y) dy\right) dx \\
    &= \langle f(x), \langle g(y), \phi(x, y) \rangle_y \rangle_x
\end{flalign}

Now we can write down the definition of direct products between distributions,

\begin{definition}[Direct Product of Distributions]
    Let $f \in \dist(\reals^N)$ and $g \in \dist(\reals^M)$. Then $f \otimes g \in \dist(\reals^{N+M})$ defined by \[ \langle f \otimes g, \phi \rangle= \langle f(x), \langle g(y), \phi(x, y) \rangle \rangle \] valid for all $\phi \in \tfspaceD(\reals^{N+M})$
    \label{def:direct_product}
\end{definition}

If we check that the RHS of the above identity defines a linear and continuous functional, and therefore a valid distribution, then it is valid. Let's write it down in a lemma,

\begin{lemma}
    Let $g \in \dist(\reals^M)$ and $\phi \in \tfspaceD(\reals^{N+M})$ then $\psi(x) = \langle g(y), \phi(x, y) \rangle \in \tfspaceD(\reals^N)$ additionally \[ \partial^\alpha \psi(x) = \langle g(y), \partial^\alpha_x\phi(x, y)\rangle\] Furthermore, if $\phi_k \rightarrow 0$, then \[ \psi_k(x) = \langle g(y), \phi_k(x, y) \rangle \rightarrow 0\] in the space $\tfspaceD(\reals^N)$
\end{lemma}

All this is basically saying is that the inner action describes a valid test function for the outer action.

\vspace{5pt}

Let's begin by showing that $\psi$ is continuous. It is enough to show this in one direction, as the same technique can be used in turn to show it for other dimensions too. Let's fix $x_0 \in \reals^N$, and consider a sequence such that $x_k \rightarrow x_0$ with $x_k \in \reals^N$. The support of the test function can be seen to be fixed within some ball $\supp \phi(x_k, y) \subset B_b$ and from linearity, we know that for any multi-index $\beta$ that $\partial_y^\beta \phi(x_k, y) \rightarrow \partial_y^\beta\phi(x_0, y)$ uniformly in $y$. Which, due to the continuity of $g(y)$ gives,

\begin{eqnarray}
    \psi(x_k) = \langle g(y), \phi(x_k, y)\rangle \rightarrow \langle g(y), \phi(x_0, y) \rangle
\end{eqnarray}

Thus $\psi$ is sequentially continuous. All that remains is to show the two properties of the lemma.

To prove the first property, let's fix $x_0 \in \reals^N$ and take the derivative along just a single axis $e_1 = (1,0,0,...,0)$, then from the fundamental theorem of calculus,

\begin{flalign}
    \frac{\partial}{\partial x_1}\psi(x_0) &=\lim_{\epsilon \rightarrow 0} (\psi(x_0 + \epsilon e_1) - \psi(x_0)) \\
    & =\lim_{\epsilon \rightarrow 0} \left(\langle g(y), \frac{\psi(x_0 + \epsilon e_1) - \psi(x_0)}{\epsilon} \rangle \right)\\
    & = \lim_{\epsilon \rightarrow 0} \left( \langle g(y), \mu_\epsilon(y) \rangle \right)
\end{flalign}

We notice that $\mu_\epsilon(y) \rightarrow \frac{\partial \phi}{\partial x_1}(x_0, y)$ in $\tfspaceD(\reals^M)$, thus validating the first property in the lemma in a single dimension - this can be repeated for any dimension or set of dimensions.

\vspace{5pt}

Finally, let's show the second statement in the above lemma. If $\phi \rightarrow 0$ in $\tfspaceD(\reals^{N+M})$ then $\exists a, b$ such that $\supp \phi_k \subset B_a \times B_b$ then $\supp \psi_k(x) = \supp \psi(x_k, \cdot) \subset B_a$. We need to show that $\forall \alpha$

\begin{eqnarray}
    \sup_{x \in \reals^N} |\partial_x^\alpha \psi_k(x) | \rightarrow 0
\end{eqnarray}

Let's prove by contradiction. If it's not true, then $\exists \alpha_0, x_k, \epsilon_0 > 0$ such that,

\begin{eqnarray}
    |\partial_x^{\alpha_0} \psi_k(x) | \geq \epsilon_0
\end{eqnarray}

Consider a function $f_k(y) = (\partial_x^{\alpha_0}\phi_k)(x_k, y)$ for any multi-index $\beta$ we know that ,

\begin{eqnarray}
    \partial_x^{\alpha_0} \partial_y^\beta \phi_k (x, y) \rightarrow 0
\end{eqnarray}

uniformly in $x$ and $y$ simply from the peroperties of test functions. We can parametrize for a sequence $x_k$,

\begin{eqnarray}
    \partial_x^{\alpha_0} \partial_y^\beta \phi_k (x_k, y) \rightarrow 0
\end{eqnarray}

uniformly in $y$ as $k \rightarrow \infty$. By linearity, we can swap the order of the differential operators,

\begin{flalign}
    &\partial_y^\beta f_k(y) \rightarrow 0 \\
    & f_k \rightarrow 0
\end{flalign}

in the sense of $\tfspaceD(\reals^M)$. This implies that,

\begin{flalign}
    \langle g(y), f_k(y) \rangle \rightarrow 0
\end{flalign}

which leads to a contradiction, thereby proving our lemma.

\begin{corollary}[The Map A]
    $\phi \in \tfspaceD(\reals^{N+M})$ mapped by $A\phi = \psi(x) = \langle g(y), \phi(x, y) \rangle$ is linear and continuous from $\tfspaceD(\reals^{N+M})$ to $\tfspaceD({\reals^N})$.
\end{corollary}

Now that we've defined products on distributions, let's examine some basic properties that will be useful for us,

\subsubsection*{1. Commutativity}

If $f \in \dist(\reals^N)$ and $g \in \dist(\reals^M)$ then $f(x) \otimes g(y) = g(y) \otimes f(x)$. Let's show this by considering the following dual test function $\phi(x, y) = \underset{1 \leq j \leq T}{\sum}u_j(x)v_j(y)$ where $u_j \in \tfspaceD(\reals^N)$ and $v_j \in \tfspaceD(\reals^M)$. Then using linearity and the properties of the direct product,


\begin{flalign}
    \langle f(x) \otimes g(y), \phi \rangle &= \underset{1 \leq j \leq T}{\sum} \langle f(x), \langle g(y), u_j(x)v_j(y) \rangle_y \rangle_x \\
    &= \underset{1 \leq j \leq T}{\sum} \langle f(x), u_j(x) \langle g(y), v_j(y) \rangle \rangle \\
    &= \underset{1 \leq j \leq T}{\sum} \langle f(x) , u_j(x) \rangle\langle g(y) , v_j(y) \rangle \\
    &= \underset{1 \leq j \leq T}{\sum} \langle g(y), \langle f(x), u_j(x) \rangle_x v_j(y) \rangle_y \\
    &= \underset{1 \leq j \leq T}{\sum} \langle g(y), \langle f(x), u_j(x)v_j(y) \rangle\rangle \\
    &= \langle g(y) \otimes f(x), \phi \rangle
\end{flalign}

To extend this to \textbf{any} test functions, and not just those of the special form above, we need to show that the set of test functions of that form are \textbf{dense} in $\tfspaceD(\reals^{N+M})$

\begin{definition}[Dense]
    $\forall \psi \in \tfspaceD(\reals^{N+M}) \exists \phi_n \in \tfspaceD(\reals^{N+M})$ of the form $\phi_n(x, y) = \underset{1 \leq j \leq T}{\sum}u_{nj}(x)v_{nj}(y)$ such that $\phi_n\overset{\tfspaceD}{\rightarrow}\psi$
\end{definition}

We basically have to prove that a mapping exists using a sequence of test functions of the special form, to any test function in the space of test functions. If this is the case, then our commutativity result above applies to arbitrary test functions from the space of test functions.

\begin{theorem}[Weierstrass Theorem]
    If $\tilde{\Omega}$ is a bounded domain and $\bar{\Omega} \subset \tilde{\Omega}$ then $\forall f \in C^m(\tilde{\Omega})$ $\forall \epsilon > 0$ $\exists$ a polynomial $P$ such that \[ \| \partial^\alpha f - \partial^\alpha P\|_{C(\Omega)} < \epsilon\] $\forall |\alpha| \leq m$
    \label{thm:weierstrass}
\end{theorem}

The above theorem basically tells us that we can approximate our test functions and their derivatives using polynomials to an arbitrary degree of precision. Why do we need this? Well, it means that if we can approximate our test function as a sequence of polynomials, which can always be written in the nice separable form that allows us to use our commutativity result for arbitrary test functions. Thus we arrive at the following lemma,

\begin{lemma}
    $\forall \phi \in \tfspaceD(\reals^{N+M})$ there exists a sequence of test functions from $\tfspaceD(\reals^{N+M})$ of the form \[\phi_n(x, y) = \underset{1 \leq j \leq T}{\sum}u_{nj}(x)v_{nj}(y)\]  where $u_{jn} \in \tfspaceD(\reals^N)$ and $v_{jn} \in \tfspaceD(\reals^M)$ which converges to $\phi$ in the sense of $\tfspaceD(\reals^{N+M})$.
\end{lemma}

Let's now apply it in a proof. Suppose that $\supp \phi \subset B_R \times B_R$. Then by Weierstrass' theorem $\exists$ a sequence of polynomials $P_k(x, y)$ with $k=1,2,...$ such that

\begin{equation}
    |\partial^\alpha \phi(x, y) - \partial^\alpha P_k(x, y) | < \frac{1}{k}
\end{equation}

where $|\alpha| \leq k$ in a larger domain, let's say $B_{3R} \times B_{3R}$, let's pick two convenient test functions, $\zeta \in \tfspaceD(\reals^N)$ and $\eta \in \tfspaceD(\reals^M)$ with the properties that they are equal to 1 inside a domain of $B_{2R}$ and equal to 0 outside of it. Then, we obstain a sequence that satisfies the compact support property of test functions, and is separable as required to show our lemma,

\begin{eqnarray}
    \phi_k(x, y) = \zeta(x)\eta(y)P_k(x, y)
\end{eqnarray}

as $\supp \phi_k \subset B_{2R} \times B_{2R}$ and $\forall \alpha$ and $k \geq |\alpha|$. We can then say that $\phi_k \overset{\tfspaceD}{\rightarrow} \phi$. So, as we can approximate a test function as a sequence of polynomials, the commutativity property follows for general test functions,

\begin{flalign}
    \langle f(x) \otimes g(y), \phi \rangle &= \lim_{k \rightarrow \infty}  \langle f(x) \otimes g(y), \phi_k \rangle \\
    & = \lim_{k \rightarrow \infty}  \langle g(y) \otimes f(x), \phi_k \rangle \\
    & = \langle g(y) \otimes f(x), \lim_{k \rightarrow \infty} \phi_k \rangle \\
    & = \langle g(y) \otimes f(x), \phi \rangle
\end{flalign}


\subsubsection*{2. Translation}

For $f \in \dist(\reals^N)$ and $g \in \dist(\reals^M)$ and $h \in \reals^N$ then,

\begin{eqnarray}
    (f \otimes g) (x+h, y) = f(x+h) \otimes g(y)
\end{eqnarray}

\subsubsection*{3. Derivatives}
\begin{eqnarray}
    \partial_y^\beta\partial_x^\alpha [f(x) \otimes g(y)] = (\partial_x^\alpha f(x)) \otimes (\partial_y^\beta g(y))
\end{eqnarray}

\subsubsection*{4. Associativity}

\begin{eqnarray}
    (f \otimes g) \otimes h = f \otimes (g \otimes h)
\end{eqnarray}

\subsubsection*{5. Multiplication by $C^\infty$ function}

If $a \in C^\infty(\reals^N)$ then

\begin{flalign}
    a(x) [f(x) \otimes g(y)] = \left(a(x) f(x) \right) \otimes g(y)
\end{flalign}

\subsubsection*{6. Identity}

Consider the distribution $f(x) \otimes 1(y)$, by commutativity

\begin{flalign}
    \langle f(x) \otimes 1(y), \phi \rangle &= \langle f(x), \langle 1(y), \phi \rangle \rangle = \langle f(x), \int_{\reals^M} \phi(x, y) dy \rangle \\
    &\equiv \langle 1(y) \otimes f(x), \phi \rangle = \int_{\reals^M} \langle f(x), \phi(x, y) \rangle dy
\end{flalign}

We say that the above distribution is  `independent' of $y$, allowing one to `pick out components' of the distribution.

\section{Convolution of Distributions}

We need one more property defined on distributions before we can rigorously define the operations we used in order to solve the Poisson problem using the knowledge of the fundamental solution. This final property is a `convolution',

\begin{definition}
If $f, g \in \tfspaceD(\reals^N)$ then the convolution of $f$ and $g$ is defined as \[ (f*g) (x) = \int_{\reals^N} f(x-y) g(y) dy \] The convolution $f*g$ defines a regular distribution on $\tfspaceD(\reals^N)$ via the rule,
\[ \langle f * g, \phi \rangle = \int (f * g) (x) \phi(x) dx =  \int \phi(x) \left(\int f(x-y) g(y) dy\right) dx\]
By a version of Fubini's theorem (see below) we can swap the integral order, giving \[ \int \int \phi(x) f(x-y) g(y) dx dy\] Using a change of variables \[ \int \int f(x) g(y) \phi(x+y) dx dy = \langle f(x) \otimes g(y), \phi(x+y) \rangle \] where $\phi \in \tfspaceD(\reals^N)$
\end{definition}

\begin{theorem}[Fubini's Theorem]
    Let $F \in \tfspaceD(\underset{x}{\reals^N} \times \underset{y}{\reals^N})$.
    Then \[ \int_{\reals^{N+N}} F(x, y) dx, dy  = \int_{\reals^N} \left(\int_{\reals^N} F(x, y) dy\right) dx\] or equivalently due to linearity,
    \[ \int_{\reals^N} \left(\int_{\reals^N} F(x, y) dx\right) dy\]

\end{theorem}

Let's prove that convolution of distributions also exists, and does indeed define a valid functional. We begin in a slightly abstract manner, consider $\eta_k \in \tfspaceD(\reals^N)$ and say that it converges to 1 in $\reals^N$ if :

\begin{enumerate}
    \item $\forall K$ - compact sets $\exists$ a number $N=N(k)$ such that $\eta_k(x) = 1$ for $x \in K$, where $k \geq N$
    \item $|\partial^{\alpha}\eta_k(x)| < C_\alpha$, $x \in \reals^N$, $k = 1, 2, ...$
\end{enumerate}


Such a sequence exists, e.g. $\eta_k(x) = \eta(x/k)$, where $\eta \in \tfspaceD, \> \eta(x) = 1, \> |x| < 1$.

Let's show that the convolution of two test functions can be rewritten as,

\begin{eqnarray}
    \langle f * g, \phi \rangle = \lim_{k \rightarrow \infty} \langle f(x) \otimes g(y), \phi(x+y)\eta_k(x, y) \rangle
\end{eqnarray}

where $\phi \in \tfspaceD(\reals^N)$ and $\eta_k(x, y) \in \tfspaceD(\reals^{2N})$ such that $\eta_k \rightarrow 1$ in $\tfspaceD(\reals^{2N})$. Then the product, $\phi \eta \in \tfspaceD(\reals^{2N}) $. Now consider again the convolution definition,
we notice that $\supp f(x) g(y) \subset \bar{B_R} \subset \reals^{2N}$, therefore by the above theorem $\exists N, \> \> \forall k > N$ such that $\eta_k = 1$ in $\bar{B_R}$ such that $f(x)g(y)\eta_k(x, y) = f(x)g(y)$ for sufficiently large $k$. Therefore, we can write,

\begin{equation}
    \langle f(x) \otimes g(y), \eta_k(x, y)\phi(x+y) \rangle
\end{equation}

for our convolution of two test functions. Now we are ready to define the convolution of distributions

\begin{definition}[Convolution of Distributions]
    Suppose $f, g \in \dist(\reals^N)$ are such that $f \otimes g$ admits, \[ \langle f \otimes g, \phi(x+y) \rangle \] For test functions of the form $\phi(x+y) \in \tfspaceD(\reals^N)$, $\forall$ sequences $\{\eta_k\} \in \tfspaceD(\reals^{2N})$ such that $\eta_k \rightarrow 1$ in $\reals^{2N}$ there exists a limit \[ \lim_{k\rightarrow \infty} \langle f(x) \otimes g(y), \eta_k(x, y)\phi(x+y) \rangle \] And the limit does not depend on the specific sequence, but only on the convergence properties. Then the convolution $f*g$ is the functional,
    \[ \langle f*g, \phi \rangle =  \lim_{k\rightarrow \infty} \langle f(x) \otimes g(y), \eta_k(x, y)\phi(x+y) \rangle\ \]
\end{definition}

Let's now show that the convolution of two distributions is in fact a valid functional in $\dist(\reals^N)$ by checking the continuity of linear functionals.

Consider,

\begin{equation}
    \langle f(x) \otimes g(y), \eta_k(x, y)\phi(x+y) \rangle
\end{equation}


Let $\phi_j \overset{\tfspaceD(\reals^N)}{\rightarrow} 0$, then $\eta_k(x, y)\phi_j(x+y) \overset{\tfspaceD(\reals^{2N})}{\rightarrow} 0$ as $j \rightarrow \infty$ ($k$ is fixed). And, since $ f otimes g$ is continuous on the dual space, we see that,

\begin{eqnarray}
    \langle f \otimes g, \eta_k \phi(x+y) \rangle \rightarrow 0
\end{eqnarray}

Therefore the convolution of two distributions is continuous, and can be seen to be a distribution itself.

\subsection{Example of Convolution}

$f \in \dist(\reals^N)$, $\delta \in \dist(\reals^N)$, what is $f * \delta$?
Take a test function $\phi \in \tfspaceD(\reals^N)$, then

\begin{flalign}
    \langle f*\delta, \phi \rangle = \lim_{k \rightarrow \infty} \langle f(x) \otimes \delta(y), \phi(x+y)\eta_k(x,y) \rangle \\
    \implies \langle f(x), \langle \delta(y), \phi(x+y) \eta_k(x, y) \rangle \rangle \\
     = \langle f(x), \phi(x) \eta_k(x, 0) \rangle
\end{flalign}

We know that $\eta_k \rightarrow 0$ in $\reals^{2N}$ implies that $\exists N$ such that $\eta_k(x, 0) = 1$ on $\supp \phi$ for $k > N$,

\begin{flalign}
    = \langle f(x), \phi(x) \rangle \\
    \implies f* \delta = f
\end{flalign}

\begin{theorem}
    Let $f, g \in \dist(\reals^N)$ and $\supp g$ is bounded, then $f * g$ exists and can be written as \[ \langle f*g, \phi \rangle = \langle f(x) \otimes g(y), \eta(y) \phi(x+y) \rangle \] $\forall \phi \in \tfspaceD(\reals^N)$ where $\eta \in \tfspaceD(\reals^N)$ such that $\eta \equiv 1$ on a neighbourhood of $\supp g$
\end{theorem}

\subsection{Properties of Convolution}

\subsubsection*{1. Commutativity}

Let $f, g \in \dist$, then if $f*g$ exists, then $g*f$ exists and $f*g = g*f$. To prove this statement, consider the action of the convolution,

\begin{eqnarray}
    \langle f*g, \phi \rangle = \lim_{k\rightarrow \infty} \langle f(x) \otimes g(y), \eta_k (x, y)\phi(x+y) \rangle
\end{eqnarray}

From the commutativity of the direct product,

\begin{flalign}
    &= \lim_{k\rightarrow \infty} \langle g(y) \otimes f(x), \eta_k (x,y)\phi(x+y) \rangle \\
    &=\lim_{k\rightarrow \infty} \langle g(y) \otimes f(x), \eta_k (y, x)\phi(x+y) \rangle \\
    & = \langle g * f, \phi \rangle
\end{flalign}

we're allowed to `swap' arguments of $\eta_k$ since $\eta_k(x, y) \rightarrow 1$ implies that $\eta_k(y, x) \rightarrow 1$ in $\reals^{2N}$

\subsubsection*{2. Translation}

If $f*g$ exists then so does $f(x+h)*g(x)$ where $h \in \reals^N$ and

\begin{eqnarray}
    f(x+h)*g(x) = (f*g) (x+h)
\end{eqnarray}

\subsubsection*{3. Reflection}

If $f*g$ exists then so does $f(-x)*g(-x)$ and,

\begin{equation}
    f(-x)*g(-x) = (f*g)(-x)
\end{equation}

\subsubsection*{4. Differentiation}

If $f*g$ exists then so does $\partial^\alpha f * g$ and $f * \partial^\alpha g$ and

\begin{equation}
    (\partial^\alpha f) * g = \partial^\alpha (f * g)  = f * \partial^\alpha g
\end{equation}


\section{Fundamentality of Fundamental Solutions}

We are finally equipped with the tools to understand \textit{why} fundamental solutions are so important. Consider a linear differential operator with constant coefficients,

\begin{equation}
    L = \sum_{|\alpha| \leq m } a_\alpha \partial^\alpha
\end{equation}

where $a_\alpha \in \complexes$, and as before the fundamental solution is defined by the solution of the equation,

\begin{equation}
    LE = \delta
\end{equation}

Consider the general partial differential equation,

\begin{equation}
    L u = f
\end{equation}

where $f$ is a distribution with compact support. Then $E*f$ exists and therefore,

\begin{equation}
    L(E*f) = \sum_{|\alpha| \leq m} a_\alpha \partial^\alpha (E*f) = \left(\sum a_\alpha \partial^\alpha E \right) * f = \delta * f = f
\end{equation}

Thus we observe that if we know the fundamental solution, we can find a weak solution for arbitrary right hand sides by using a convolution. Of course there are the usual conditions on the right hand side, such as compact support and continuity.

\subsubsection*{Example}

Consider the Poisson problem in $\reals^3$

\begin{equation}
    \Delta u = f
\end{equation}

with $f \in \tfspaceD$, we know that $u = -\frac{1}{4\pi}\frac{1}{|x|} * f \in \dist$ solves the problem for $\forall f \in \tfspaceD$. We can actually go further, as we know that $\frac{1}{|x|} \in \lone$ and $f \in \dist$ therefore the convolution is also in $\lone$, and is a regular distribution. In fact we will soon see that it is also a $C^\infty$ function, and thus is not only a weak solution - but a solution in the classical sense too. Fundamental solutions offer us a way of generating general solutions to linear partial differential equations.

\section{Schwartz Kernel and the Kernel Theorem}

An integral transform is a map of the form,

\begin{equation}
    f \mapsto Kf = \int_Y k(x, y) f(y) dy, \> \> \> x \in X
\end{equation}

where $k(x, y)$ is the kernel of transformation, defined on the domain $X \times Y$. The convolution can be considered to be a form integral transform. Consider a more general integral transform. Let $k \in \dist(\underset{X}{\reals^N} \times \underset{Y}{\reals^M})$ then, $K : \tfspaceD(\reals^M) \rightarrow \dist(\reals^N)$. We define the operation,

\begin{equation}
    \langle K \psi, \phi \rangle \overset{\text{def}}{=} \langle k(x, y), \psi(y)\phi(x) \rangle
\end{equation}

For $\forall \phi \in \tfspaceD(\reals^N), \> \forall \psi \in \tfspaceD(\reals^M)$. The functional is clearly linear on $\tfspaceD(\reals^N)$ and it is also seen to be continuous. Therefore $K\psi \in \dist(\reals^N)$.

\vspace{5pt}

We can check the continuity of the kernel operator, $K$, in mapping from $\tfspaceD(\reals^M) \mapsto \dist(\reals^N)$. Take $\psi_n \rightarrow 0$ in $\tfspace(\reals^M)$, then we have,

\begin{equation}
    \langle K \psi_n, \phi \rangle = \langle k, \psi_n\phi \rangle \rightarrow 0
\end{equation}

since $\psi_n(y)\phi(x) \rightarrow 0$ in $\tfspace(\reals^{N+M})$, that is $K\psi_n \rightarrow 0$ in $\dist(\reals^N)$ therefore $K$ is continuous.

\begin{theorem}[Schwartz Kernel Theorem]
    A linear map $A : \tfspaceD{\reals^M} \mapsto \dist(\reals^N)$ is sequentially continuous if and only if it is generated by a kernel $a \in \dist(\reals^{N+M})$. This is called a Schwartz kernel, \[ \langle A \psi, \phi \rangle = \langle k, \phi \otimes \psi \rangle \] For all $\phi \in \tfspaceD(\reals^N)$ and $\psi \in \tfspaceD(\reals^M)$
\end{theorem}

Intuitively, this theorem states that any `reasonable' linear map has a Schwartz kernel.

\section{The Fourier Transform}

Fourier transforms will be a vital technique for solving linear partial differential equations. Furthermore, their exists numerous well known software libraries for numerically performing Fourier transforms - which make them an invaluable ally in tackling PDEs.

\vspace{5pt}

Let's start with the definition, the Fourier transform of a function $f$ on $\reals^N$ is,

\begin{flalign}
    \hat{f}(\zeta) = \int_{\reals^N} e^{-i x \cdot \zeta} f(x) dx = \mathcal{F}[f](\zeta) = \mathcal{F}_{x\rightarrow\zeta}[f(x)]
    \label{eq:ft}
\end{flalign}

where $\zeta \in \reals^N$. This operation is always well defined if $f \in \lone(\reals^N)$.

\section{Properties of Fourier Transform}

\subsubsection*{1. Continuity of Transform}

Let $f \in \tfspaceD$ then $\hat{f} \in C$. To prove this statement we need to show that $\hat{f}(\zeta_u) \rightarrow \hat{f}(\zeta)$ if a sequence converges $\zeta_u \rightarrow \zeta$, we know that $f \in \tfspaceD$ so $\supp f \subset B_R$

\begin{flalign}
    \hat{f}(\zeta_u) &= \int_{B_R} e^{-i x \cdot \zeta_n}f(x) dx \\
    & = \int_{B_R} \left( e^{-i x \cdot \zeta_n} - e^{-ix \cdot \zeta} \right) f(x) dx + \int_{B_R} e^{-ix \cdot \zeta}f(x) dx = \hat{f}(\zeta)
\end{flalign}

The first integral term converges to zero due to the convergence of the sequence $\zeta_n$, and we recognise the second term as being a Fourier transform. Thus the identity follows.

\subsubsection*{2. Convolution}

Let $f, g \in \tfspaceD$ then if $f*g \in \lone$ we find $\mathcal{F}(f*g) = \hat{f} \cdot \hat{g}$ where the multuplication is done in the usual sense, rather than in the sense of distributions. To prove this, we know that if $f*g \in \lone$ then $\widehat{f*g}$ is well defined, therefore

\begin{flalign}
    \widehat{f*g}(\zeta) &= \int_{\reals^N}e^{-ixz}(f*g)(x) dx \\
    &= \int_{\reals^N} e^{-ix \cdot \zeta} \left( \int_{\reals^N} f(x-y) g(y) dy \right) dx
\end{flalign}

Applying Fubini's theorem, and a change of variables $x = y+z$,

\begin{flalign}
    &= \int \int f(z) g(y) e^{-i(y+z)\zeta} dzdy \\
    &= \hat{f}(\zeta)\hat{g}(\zeta)
\end{flalign}

\subsubsection*{3. Associativity}
Let $f, g \in \tfspaceD$ then

\begin{equation}
    \int_{\reals^N} f(x) \hat{g}(x) dx = \int_{\reals^N} \hat{f}(\zeta) g(\zeta) d\zeta
\end{equation}

This means that,

\begin{flalign}
    \int \hat{f} g dx = \int f \hat{g} dx \\
    \int \hat{f} \phi dx = \int f \hat{\phi} dx \\
    \langle \hat{f}, \phi \rangle = \langle f, \hat{\phi} \rangle
\end{flalign}

The proof for this final property, can be seen from a direct calculation of the left hand side.

\section{Schwartz Space}

Fourier Transofrms do not preserve the properties of test functions,

\begin{eqnarray}
    \phi \in \tfspaceD \text{ then generally } \mathcal{F}\phi \notin \tfspaceD
\end{eqnarray}

We need a new space which we can map our test functions to in order to perform Fourier transforms on them.

\begin{definition}
    A function $\phi \in \tfspace(\reals^N)$ is called \textbf{rapidly decreasing}, and $\phi \in S$, if \[ \| \phi \|_{\alpha, \beta} = \sup_{x \in \reals^N}|x^\alpha \partial^\beta \phi| < + \infty\] For all $\alpha, \beta$. $S$ is called a Schwartz space.
\end{definition}

Note convergence in the sense of $S$ means,

\begin{flalign}
    \phi_j \overset{S}{\rightarrow} 0 \text{ if } \|\phi_j\|_{\alpha, \beta} \rightarrow 0
\end{flalign}

for all $\alpha, \beta$.

\section{Properties of Schwartz Space}

\subsubsection*{1. When multiplying/differentiating remain in $S$}

$\forall \phi \in S$, $x^\alpha \partial^\beta \phi \in S$ and the map $\phi \rightarrow x^\alpha \partial^\beta \phi$ is continuous from $S$ to $S$.

This statement is obvious, and entirely follows from the above definitions.

\subsubsection*{2. $\tfspaceD \subset S$ and injection is continuous}

Clearly $\tfspaceD \subset S$ must be the case, the continuous injection means that the identity map from $\tfspaceD \mapsto \tfspaceD$ is continuous as map $\tfspaceD \mapsto S$. Thus,

\begin{equation}
    \phi_n \overset{\tfspaceD}{\rightarrow} 0 \implies   \phi_n \overset{S}{\rightarrow} 0
\end{equation}

\subsubsection*{3. $\tfspaceD$ is dense in $S$}

This basically means that $\forall \phi \in S$ $\exists \phi_n \in \tfspaceD$ such that $\phi_n \overset{S}{\rightarrow} \phi$. Take $\phi \in S$ and consider $\rho \in \tfspaceD : \rho = 1$ in $B_1$ (centered at the origin) and set $\phi_j(x) = \rho(\frac{x}{j})\phi \in \tfspaceD$ for $j=1,2...$ We calculate,

\begin{flalign}
    \|\phi_j - \phi\|_{\alpha, \beta} &= \sup_{x \in \reals^N} |x^\alpha \partial^\beta \phi_j - x^\alpha\partial^\beta\phi| \\
    & =  \sup_{x \in \reals^N, |x| > j} |x^\alpha \partial^\beta \phi_j - x^\alpha\partial^\beta\phi|
\end{flalign}

The second equality comes from the fact that $\rho(\frac{x}{j}) = 1$ when $|x|>j$ which implies that $\phi_j = \phi$ for $|x|>j$. Then, by Cauchy-Schwarz,


\begin{flalign}
    \leq \sup_{|x|>j} |x^\alpha\partial^\beta\phi| + \sup_{|x|>j} |x^\alpha\partial^\beta\rho(\frac{x}{j}) \phi|
\end{flalign}

Both of these terms converge to zero. Consider the first term,

\begin{flalign}
    \sup_{|x|>j} |x^\alpha\partial^\beta\phi| &= \sup_{|x|>j} |x|^{-2} | |x|^2 x^\alpha \partial^\beta \phi | \\
    &\leq j^{-2} \|\phi\|_{\gamma, \beta}
\end{flalign}

where $\gamma = \alpha + (2,2,...,2)$. This semi-norm is finite as $\phi \in S$, therefore the term converges to zero as $j \rightarrow \infty$. The second term similarly converges to zero.

\subsubsection*{4. $S \subset \lone$ and the injection is continuous}

Take $\phi \in S$, then,

\begin{flalign}
    \int_{\reals^N} |x|dx &= \int_{\reals^N} (1 + |x|^2)^{-M}  (1 + |x|^2)^{M} |\phi| dx \\
    &\leq \sup_{x \in \reals^N} (1+|x|^2)^M|\phi| \times \int_{\reals^N}(1 + |x|^2)^{-M} dx
\end{flalign}

The integral on the secon line above will be finite and bounded as long as $M>0$, we can encode this in a constant,

\begin{flalign}
    &\leq C \sup_{x \in \reals^N} (1+|x|^2)^M |\phi| \\
    &\leq C \tilde{C} \sum_{|\alpha| \leq 2M} \|\phi\|_{\alpha, 0}
\end{flalign}

We see therefore that the integral is finite, and that $\phi \in S$ means that $\phi \in \lone$. The continuity of the injection, i.e.

\begin{flalign}
    \phi_n \overset{S}{\rightarrow} 0 \\
    \implies \phi_n \overset{\lone}{\rightarrow} 0
\end{flalign}

The convergence in $\lone$ implies that $\int |\phi_n| dx \rightarrow 0$, which provides the reverse impliciation over $S$ too, and we know now that $S \subset \lone$.

\section{Fourier Transform in Schwartz Space}

We want the following property from our Schwartz space as it undergoes a Fourier transform.

\begin{flalign}
    S \overset{\mathcal{F}}{\leftrightarrow} S
\end{flalign}

In more technical language, we want the Fourier transform to be a structure preserving isomorphism between elements of a Schwartz space.

\begin{lemma}
    $\phi \in S$ implies that,

    \begin{enumerate}
        \item $\widehat{\partial^\alpha\phi} = (i \zeta)^\alpha \hat{\phi}$
        \item $\widehat{x^\alpha\phi} = (-i)^{-|\alpha|}\partial^\alpha \hat{\phi}$
    \end{enumerate}

    Furthermore, the Fourier transform exist by (1) and (4) of the above properties of Schwartz spaces.
\end{lemma}

To prove the above lemma, we can see that (1) is a straightforward application of integration by parts from the definition of the Fourier transform, and (2) follows from the differentiation properties of a distribution under an integral.

\begin{lemma}
    $\mathcal{F}$ is a continuous map from $S$ to $S$.
\end{lemma}

Let $\phi \in S$, then $\hat{\phi} \in C^\infty$. We can see this from the fact that $\partial^\alpha \hat{\phi} \sim \widehat{x^\alpha\phi}$, $x^\alpha\phi \in S \subset \lone$, so $\widehat{x^\alpha\phi}$ exists and is continuous as functions in $\lone$ have well defined Fourier transforms. Therefore $\partial^\alpha \hat{\phi} \in C(\reals^N)$, for arbitrary $\alpha$, which implies that $\hat{\phi} \in C^\infty$. Secondly, if $\phi_n \overset{S}{\rightarrow} 0$, we need to show that $\hat{\phi_n} \overset{S}{\rightarrow} 0$, let's take its semi-norm,

\begin{flalign}
    \|\hat{\phi_n}\|_{\alpha, \beta} &= \sup_{\zeta \in \reals^N} |\zeta^\alpha \partial^\beta\hat{\phi_n}| = \sup_{\zeta \in \reals^N} |\zeta^\alpha \widehat{x^\beta\phi_n}|\\
\end{flalign}

By the first lemma,

\begin{flalign}
    =\sup_{\zeta \in \reals^N} |\widehat{\partial^\alpha x^\beta \phi_n}| \leq \|\partial^\alpha x^\beta \phi_n\|_{\lone}
\end{flalign}

The final inequality can be seen from an application of the definition of a Fourier transform,

\begin{flalign}
    |\hat{\phi}(\zeta)| = |\int e^{-ix\zeta} \phi(x) dx| &\leq \sup_{x \in \reals^N} |e^{ix\zeta}| \int |\phi | dx =\sup_{x \in \reals^N} |e^{ix\zeta}|\cdot  \|\phi\|_{\lone}  \\
    &\implies \sup_{\zeta \in \reals^N} |\hat{\phi(\zeta)}| \leq \|\phi\|_{\lone}
\end{flalign}

Thus as the Fourier transform has a finite semi-norm, it can be seen to also be a member of the Schwartz space.

\begin{lemma}
    $\mathcal{F}e^{-\frac{|x|^2}{2}} = (2\pi)^{\frac{N}{2}}e^{-\frac{|\zeta|^2}{2}}$ where $x, \zeta \in \reals^N$.
\end{lemma}

We see that the Gaussian function in a Schwartz space is an eigen-function of the Fourier transform. Let's show this in the one-dimensional case, where $\phi(x) = e^{-\frac{x^2}{2}}$, clearly this satisfies the property that $\partial_x \phi = -x \phi$

\begin{flalign}
    \widehat{-x\phi} = \widehat{\partial_x \phi} \\
    \implies \frac{1}{i} \partial_z \hat{\phi}= i \zeta \hat{\phi} \\
\end{flalign}

Which implies that $\hat{\phi}(\zeta) = C e^{-\frac{\zeta^2}{2}}$. Let us calculate $C = \hat{\phi}(0)$, by definition,

\begin{flalign}
    C = \hat{\phi}(0) &= \left [ \int_{\reals} e^{-ix\zeta} e^{-\frac{x^2}{2}} dx \right]_{\zeta = 0} \\
    &= \int_{\reals} e^{-\frac{x^2}{2}} dx = (2\pi)^{\frac{1}{2}}
\end{flalign}

\section{Fourier Inversion Theorem}

\begin{theorem}
    If $\phi \in S$ then, \[ \phi(x) = (2\pi)^{-N} \int_{\reals^N} \hat{\phi}(\zeta) e^{ix\zeta}d\zeta\]
\end{theorem}

We now that there's a property of Fourier transforms on $\tfspaceD$ such that,

\begin{flalign}
    \langle \hat{\phi}, \psi \rangle = \langle \phi, \hat{\psi} \rangle
\end{flalign}

where $\psi, \phi \in \tfspaceD$. However as the argument above was based on Fubini's theorem, the identity still applies in $S$. Let $\epsilon$ be a positive number and take $\psi(\zeta) = e^{-\frac{\epsilon^2}{2}|\zeta|^2}$, then it follows that,

\begin{flalign}
    \int \hat{\phi}(\zeta) e^{-\frac{\epsilon^2}{2}|\zeta|^2} dz &= \int \phi(x) \widehat{e^{-\frac{\epsilon^2}{2}|\zeta|^2}} d\zeta \\
    &= (2\pi)^{\frac{N}{2}} \epsilon^{-N} \int \phi(x) e^{-\frac{|x|^2}{2\epsilon^2}}dx \\
    &= (2\pi)^{\frac{N}{2}} \int \phi(\epsilon x) e^{-\frac{z^2}{2}} dz
\end{flalign}

The final line works via a change of variables, and shoving the epsilon power into some function $\phi(\epsilon x)$. Using the dominated convergence theorem, as $\epsilon \rightarrow 0$ we get,

\begin{flalign}
    \int \hat{\phi}(\zeta) d\zeta = (2\pi)^{\frac{N}{2}} \phi(0) \int e^{-\frac{x^2}{2}} dx \\
    \implies \phi(0) = (2\pi)^{-N} \int \hat{\phi}(\zeta) d\zeta
\end{flalign}

take $h \in \reals^N$, then

\begin{flalign}
    \widehat{\phi(x+h)} &= \int e^{-ix\zeta}\phi(x+h) dx \\
    &= \int e^{i\zeta h - ix \zeta}\phi(x) dx = e^{i\zeta h} \hat{\phi}(\zeta)
\end{flalign}

Then we see that,

\begin{flalign}
    \phi(h) = (2\pi)^{-N} \int e^{i\zeta h} \hat{\phi}(\zeta) d\zeta
\end{flalign}

$\forall h \in \reals^N$.

\vspace{5pt}

\begin{theorem}[Dominated Convergence Theorem]
    Assume $f_n \in \lone(\reals^N)$ and $ f_n \rightarrow f$ almost everywhere. Then suppose that $\exists g \in \lone(\reals^N)$ such that, \[ |f_n| \leq g\] almost everywhere, then $f \in \lone(\reals^n)$ and \[ \int_{\reals^N} f_n dx \rightarrow \int_{\reals^N} f dx\]
\end{theorem}

\section{Tempered Distributions}

\begin{definition}[Tempered Distribution]
    A tempered distribution is a linear continuous functional on $S$. We denote it by $S'$.
\end{definition}

\begin{lemma}
    If $f_k \in S'$ we say $f_k \overset{S'}{\rightarrow} 0$ if, \[ \langle f_k, \phi \rangle \rightarrow 0 \] $\forall \phi \in S$
\end{lemma}

\begin{lemma}
    $D \subset S \implies S' \subset D'$
\end{lemma}

\subsubsection*{1.}
A function $f$ is called a tempered function in $\reals^N$ if for some $m \geq 0$,

\begin{equation}
    \int_{\reals^N} |f| (1 + |x|)^{-m} dx < +\infty
\end{equation}

for example $e^x \in \lone$ but isn't a tempered distribution, as the above integral doesn't converge. A tempered function defines a tempered distribution, in an analogous way to which a regular function defines a regular distribution, with the action

\begin{equation}
    \langle f, \phi \rangle = \int_{\reals^N} f(x) \phi(x) dx
\end{equation}

For $\forall \phi \in S$.

\subsubsection*{2.}

If $f \in \dist$ and $f$ has compact support, then $f \in S'$ and,

\begin{equation}
    \langle f, \phi \rangle_{S'} = \langle f, \eta \phi \rangle_{D'}, \> \> \> \forall \phi \in S
\end{equation}

where $\eta \in D$ and $\eta = 1$ in a neighbourhood of $\supp f$

If

\begin{equation}
    \phi_S \implies \langle f, \underset{\in D}{\eta \phi} \rangle
\end{equation}

and $\phi_k \overset{S}{\rightarrow} 0$ then $\eta \phi_k \rightarrow 0$ implying that $\langle f, \eta \phi_k \rangle \rightarrow 0$, thefore by definition of $S'$ implying that $\langle f, \phi \rangle_{S'}$ does indeed define elements of $S'$. This fact is independent of the choice for $\eta$.

\subsubsection*{3.}

If $f \in S'$, then $\partial^\alpha f \in S'$,

\begin{equation}
    \langle \partial^\alpha f, \phi \rangle = (-1)^{|\alpha|} \langle f, \partial^\alpha \phi \rangle, \> \> \> \forall \phi \in S
\end{equation}

This can be seen from the fact that $\partial^\alpha \phi \in S$ and the partial differential operator is a continuous map within $S$.

\subsubsection*{4.}

If $f \in S'$ and $A$ is a real non-singular $N \times N$ matrix, and $b \in \reals^N$, then $f(Ax+b) \in S'$ defined by,

\begin{equation}
    \langle f(Ax+b), \phi \rangle = \langle f, \frac{1}{|\text{det}(A)|}\phi(A^{-1}(x-b))\rangle, \> \> \> \forall \phi \in S
\end{equation}


\subsubsection*{5.}

If $f \in S'$ and $a \in O_M$ then $af \in S'$ and,

\begin{equation}
    \langle af, \phi \rangle = \langle f, a\phi \rangle, \> \> \> \forall \phi \in S
\end{equation}

where $O_M$ is called the set of multipliers in $S$. This means that if $a \in O_M$, $a$ is a $C^\infty$ function and,

\begin{equation}
    |\partial^\alpha a| \leq C_\alpha (1+|x|)^{m_\alpha}
\end{equation}

One can check that if $\phi \in S$ and $a \in O_M$, then $a\phi \in S$ and the operation $\phi \mapsto a \phi$ is linear and continuous within $S$, implying that the above identity is well defined.

\subsubsection*{6.}

$e^x \in \lone$ implies that $e^x \in \dist$, but $e^x \notin S'$

\begin{theorem}
    A linear functional $f$ on $S$ belongs to $S'$ if and only if,
    \[ \exists C > 0, N \geq 0 \]
    such that
    \[ \langle f, \phi \rangle \leq C \sum_{|\alpha|,|\beta| \leq N} \|\phi|\|_{\alpha, \beta}\]
    $\forall \phi \in S$
\end{theorem}

\section{Fourier Transform on $S'$}

If $f \in \lone$ then by theorem $\hat{f}$ is a continuous and bounded function, which implies that it defines a regular distribution on $S$ via the formula,

\begin{flalign}
    \langle \hat{f}, \phi \rangle = \int_{\reals^N} \hat{f}(\zeta)\phi(\zeta)d\zeta = \int_{\reals^N} f(x) \hat{\phi}(x) dx = \langle f, \hat{\phi} \rangle, \> \> \> \forall \phi \in S
\end{flalign}

we take this identity as the definition of $\hat{f}$.

\begin{definition}
    The Fourier transform of $f \in S'$ is $\hat{f} \in S'$ given by,
    \[ \langle \hat{f}, \phi \rangle = \langle f, \hat{\phi} \rangle \]
    $\forall \phi \in S$. This defines a linear continuous functional on $S$.
\end{definition}

As usual, the `linearity' refers to the fact that the Fourier transform is a linear map, and the `continuity' within $S$ refers to the fact that $\phi_n \overset{S}{\rightarrow} 0$ implies $\hat{\phi}_n \overset{S}{\rightarrow} 0$, and therefore that $\langle \hat{f}, \phi_n \rangle \rightarrow 0 \implies \langle f, \hat{\phi}_n \rangle \rightarrow 0$.

\subsubsection*{Example 1}

$\delta \in S'$, what is $\hat{\delta}$?

\begin{flalign}
    \langle \hat{\delta}, \phi \rangle &= \langle \delta, \hat{\phi} \rangle \\
    &= \hat{\phi}(0) \\
    &= \left [  \int_{\reals^N} e^{-ix\zeta}\phi(x) dx \right ]_{\zeta = 0} \\
    & = \int_{\reals^N}\phi(x) dx = \langle 1, \phi \rangle, \> \> \forall \phi \in S \\
    \implies \hat{\delta} = 1
\end{flalign}

\subsubsection*{Example 2}
In $\reals$, what is $\widehat{H(-x)}$?

\begin{equation}
    H(-x) = \twopartdef{0}{x > 0}{1}{x \leq 0}
\end{equation}

applying the definition,

\begin{eqnarray}
    \widehat{H(-x)}(\zeta) = \int_{\reals} e^{-ix\zeta}H(-x)dx = \int_{-\infty}^0e^{-ix\zeta}dx = \infty
\end{eqnarray}

We see that we get an unbounded integral for the Fourier transform, however we can fix this. Consider $f_t(x) = H(-x)e^{tx}$. $f_t \in \lone$ and $f_t \rightarrow H(-x)$ is continuous in $S'$ by the dominated convergence theorem. Then,

\begin{flalign}
    \hat{f}_t = \int_{\reals} e^{-ix\zeta}H(-x)e^{tx}dx = \int_{-\infty}^0 e^{-ix\zeta}e^{tx}dx = \frac{1}{t-i\zeta} = \frac{i}{\zeta + it}
\end{flalign}

See problem sheet two for more information, but it can be shown that as $\hat{f}_t \rightarrow \widehat{H(-x)}$, the Fourier transform is well-defined and converges.

\begin{remark}
    If $f_t \overset{S'}{\rightarrow} H(-x)$ and $\mathcal{F}$ is continuous from $S'$ to $S'$ \[\implies \hat{f}_t \rightarrow \widehat{H(-x)} \]
\end{remark}

\section{Invertibility of FT on $S'$, \& Operational Rules}

\begin{theorem}
    The Fourier transform is an isomorphism from $S'$ to $S'$ $\mathcal{F}$ and $\mathcal{F}^{-1}$ are continuous from $S'$ to $S'$.
\end{theorem}

\subsubsection*{Proof}
The fact that the Fourier transform is linear is obvious from its definition. For continuity of the forward transform, consider $f_n \rightarrow 0$ in $S'$ then,

\begin{equation}
    \langle \mathcal{F} f_n, \phi \rangle \overset{\text{def}}{=} \langle f_n, \mathcal{F} \phi \rangle \rightarrow 0
\end{equation}

since $\hat{\phi} \in S$ and $f_n \overset{S'}{\rightarrow} 0$, giving our result. For the inverse transform, we start by considering a map $PL S' \mapsto S'$, given By

\begin{equation}
    P[f(x)](\zeta) = \frac{1}{(2\pi)^N} \mathcal{F}[f(-x)]
\end{equation}

this is a form of reflection, let's show that

\begin{flalign}
    P\mathcal{F}[f] = f
\end{flalign}

and that

\begin{flalign}
    \mathcal{F}P[f] = f, \> \> \forall f \in S'
\end{flalign}

if we do this, we can identify $P$ as the inverse Fourier transform. We have $\forall \phi \in S$,

\begin{flalign}
    \langle P \mathcal{F}[f], \phi \rangle &= \langle P \hat{f}, \phi \rangle \\
    &= \frac{1}{(2\pi)^N} \langle \mathcal{F}[\hat{f}(-\zeta)], \phi \rangle\\
    &= \frac{1}{(2\pi)^N} \langle \hat{f}(-\zeta), \hat{\phi}(\zeta)\rangle \\
    &= \langle \hat{f}(-\zeta), \mathcal{F}^{-1}[\phi](\zeta) \rangle \\
    &= \langle f, \mathcal{F}\mathcal{F}^{-1}[\phi] \rangle = \langle f, \phi \rangle \implies P \mathcal{F} f = f
\end{flalign}

similarly, we can also show the other identity $\forall f \in S'$. If $\mathcal{F} f = 0$, then $f=0$, and $\forall f \in S'$ $\exists g = \mathcal{F}^{-1}f$ such that $\mathcal{F}g = f$. This is a one-to-one mapping, an isomorphism, and $\mathcal{F}^{-1}$ is continuous which is obvious as it's the composition of two continuous operations, the forward transform and a reflection.

\begin{lemma}[Operational Rules]
    $\forall u \in S', \> \> \forall h \in \reals^N$,
    \[(1) \> \> \> \> \widehat{\partial^\alpha u} = (i \zeta)^\alpha \hat{u}\]
    \[(2) \> \> \> \> \widehat{x^\alpha u} = (-1)^{-|\alpha|}\partial^\alpha\hat{u} \]
    \[(3) \> \> \> \> \widehat{u(x+h)} = e^{i\zeta h} \hat{u}(\zeta) \]
    \[(4) \> \> \> \> \widehat{u(x)e^{ixh}} = \hat{u}(x-h)\]

\end{lemma}

\subsubsection*{Proof of OR1}

$\forall u \in S', \> \> \phi \in S$ we have,

\begin{flalign}
    \langle \widehat{\partial^\alpha u}, \phi \rangle &= \langle \partial^\alpha u, \hat{\phi} \rangle \\
    & = (-1)^{|\alpha|} \langle u, \partial^\alpha \hat{\phi}\rangle \\
    &= (-1)^{|\alpha|} \langle u, (-i)^{|\alpha|} \widehat{x^\alpha\phi}\rangle \\
    &= i^{|\alpha|} \langle \hat{u}, x^\alpha \phi \rangle = i^{|\alpha|} \langle x^\alpha \hat{u}, \phi \rangle
\end{flalign}


\subsubsection*{Example 1}

$1 \in S'$, what is $\hat{1}$?. We know that $\hat{\delta} =1$

\begin{flalign}
    \implies \mathcal{F}[1](\zeta) &= \mathcal{F}[\hat{\delta}(x)](\zeta) \\
    & = (2\pi)^N \frac{1}{(2\pi)^N}\mathcal{F}[\hat{\delta}(-x)](\zeta) \\
    & = (2\pi)^N \mathcal{F}^{-1} \hat{\delta} = (2\pi)^N \delta
\end{flalign}

\subsubsection*{Example 2}

\begin{equation}
    \mathcal{F}(x^\alpha) = (-1)^{-|\alpha|}\partial^\alpha \hat{1} =(-1)^{-|\alpha|}(2\pi)^N \partial^\alpha \delta
\end{equation}

\section{Fourier Transform of Distributions with Compact Support}

\begin{theorem}
    If $u \in \dist$ and has compact support then, $\hat{u} \in C^\infty$ and $\hat{u}(\zeta) = \langle u(x), e^{-ix\zeta}\eta(x) \rangle$ where $\eta \in \tfspaceD$ which is equal to 1 in a neighbourhood of $\supp u$.
\end{theorem}

\subsubsection*{Proof}

We know that $\forall f \in \dist(\reals^N)$ $\forall \phi \in \tfspaceD(\reals^{(N+N)})$ that $\zeta \mapsto \langle f(x), \phi(x, \zeta) \rangle \in C^\infty$, which we proved in the section on direct products,

\begin{flalign}
    \implies \forall \mu \in \tfspaceD(\reals^N) \\
    \zeta \mapsto \mu(\zeta)\hat{u}(\zeta) = \langle u(x), \mu(\zeta)e^{-ix\zeta}\eta(x) \rangle \in C^\infty
\end{flalign}

Therefore,

\begin{flalign}
    \langle u(x), e^{-ix\zeta}\eta(x) \rangle \in C^\infty
\end{flalign}

Furthermore, $\phi \in \tfspaceD$, $u \in \dist$ and $\supp u$ is compact implies that $u \in S'$,

\begin{flalign}
    \langle \hat{u}, \phi \rangle &= \langle u, \underset{\in S}{\hat{\phi}} \rangle = \langle u, \eta \hat{\phi} \rangle \\
    &= \langle u, \eta(x) \int e^{-ix\zeta} \phi(\zeta) d\zeta \rangle \\
    &= \langle u, \int \eta(x) e^{-ix\zeta} \phi(\zeta) d\zeta \rangle
\end{flalign}

Using an analog of Fubini's theorem, we can swap the integrals,

\begin{flalign}
    = \int \langle u, \eta(x) e^{-ix\zeta} \phi(\zeta) d\zeta \rangle \\
    = \int \langle u, \eta(x) e^{-ix\zeta} \rangle \phi(\zeta) d\zeta, \> \> \> \forall \phi \in \tfspaceD
\end{flalign}

Since $\tfspaceD$ is dense in $S$, we have,

\begin{equation}
    \hat{u} = \langle u, \eta(x)e^{-ix\zeta} \rangle
\end{equation}

\subsubsection*{Example}

$\delta \in \tfspaceD$, $\supp \delta = 0$,

\begin{flalign}
    \hat{\delta} = \langle \delta(x), \eta(x)e^{-ix\zeta} \rangle
\end{flalign}

$\forall \eta \in \tfspaceD$, $\eta = 1$ in neighbourhood of 0,

\begin{flalign}
    \implies \hat{\delta} = \eta(0) = 1
\end{flalign}

\section{Fourier Transform and Convolution}

\begin{theorem}
    If $u, v \in \dist$ and both have compact support then,
    \[ \widehat{u*v} = \hat{u} \hat{v} \]
\end{theorem}

\subsubsection*{Proof}

As $u * v \in \dist$ and $\supp (u*v)$ is bounded then $u*v \in S'$, therefore $\widehat{u*v}$ is well defined. For $\forall \phi \in S$,

\begin{flalign}
    \langle \widehat{u*v}, \phi \rangle & = \langle u*v, \hat{\phi} \rangle \\
    &= \langle u(x) \otimes v(y), a(x)b(y) \hat{\phi}(x+y) \rangle
\end{flalign}

where $a, b \in \tfspaceD(\reals^N)$ and both equal 1 in the neighbourhood of $\supp u$ and $\supp v$ respectively.

\begin{flalign}
    =\langle u(x) \otimes v(y), a(x)b(y) \int \phi(\zeta) e^{-i\zeta(x+y)}d\zeta \rangle
\end{flalign}

Applying the analog of Fubini's theorem twice, we can re-arrange the integrals,

\begin{flalign}
    \int \langle u(x), a(x) e^{-ix\zeta} \rangle \langle v(y), b(y) e^{-iy\zeta} \rangle \phi(\zeta) d\zeta
\end{flalign}

Using the theorem from the previous section, we see that we have our result.

Recall that,

\begin{equation}
    f \in O_M(\reals^N) \Leftrightarrow \{ f \in C^\infty : |\partial^\alpha f(x)| \leq C_\alpha (1+|x|)^{m_\alpha} \}
\end{equation}

Then the following lemma holds,

\begin{lemma}
    If $v \in \dist$ and has compact support then $\hat{v} \in O_M$
\end{lemma}

\subsubsection*{Proof}

We know that $\hat{v} \in C^{\infty}$, see the previous section, then all we need to do is show that the bound holds.

\begin{flalign}
    |\partial^\alpha_\zeta \hat{v}(\zeta) | = |\widehat{x^\alpha v(x)}| = |\langle x^\alpha v(x), \eta(x) e^{-ix\zeta} \rangle|
\end{flalign}

where $\eta \in \tfspaceD$ and $\eta = 1$ in neighbourhood of $\supp v$. Furthermore, $x^\alpha v \in S'$ as $x^\alpha$ is a multiplier, therefore we can take semi-norm estimates in the Schwartz space

\begin{flalign}
    |\partial^\alpha_\zeta \hat{v}(\zeta)| &= |\langle x^\alpha v(x), \eta(x) e^{-ix\zeta} \rangle| \\
    \leq C_\alpha \sum_{|\gamma|, |\delta| \leq N_\alpha } \|\eta(x) e^{-ix\zeta}\|_{\gamma, \delta}
\end{flalign}

where the inequality is with respect to some finite sum of semi-norms. $\eta$ has finite support,

\begin{flalign}
    \leq C_\alpha \sum_{|\gamma|, |\delta| \leq N_\alpha } \sup_{x \in \supp \eta} |x^\gamma \partial^\delta_x \eta(x) e^{-ix\zeta}| \\
    \leq \tilde{C}_\alpha \sum_{|\delta| \leq N_\alpha} \tilde{C}_\delta |\zeta^\delta| \leq C'_\alpha(1+|\zeta|^{N_\alpha})
\end{flalign}

\section{Singular Support}

\begin{definition}
    Let $u \in \dist$. The singular support of $u$ is the complement of the set,
    \[ \{ x \in \reals^N | u \in C^\infty \> \> \text{on some neighbourhood of } x\} \]
\end{definition}

For example $\singsupp \delta = 0$.

\begin{lemma}
    Let $u,v  \in \dist$ and $v$ has compact support, then
    \[ \singsupp u*v \subset \singsupp u + \singsupp v \]

\end{lemma}

\begin{remark}
    \[ A, B \subset \reals^N, \> \> \> A+B = \{ x \in \reals^N | x=a+b, a \in A, b \in B\} \]
\end{remark}

\begin{remark}
    $u \in \dist$ and $\rho \in D$ implies that $\rho * u \in C^\infty$, if $u \in \dist$ and $u$ has compact support $\rho \in C^\infty$ implies that $\rho * u \in C^\infty$
\end{remark}

\subsubsection*{Proof}

Choose $\phi \in C^\infty$ such that $\phi = 1$ on a neighbourhood of $\singsupp u$ and $\psi \in \tfspaceD$ such that $\psi = 1$ on a neighbourhood of $\singsupp v$.

\begin{flalign}
    u*v &= (\phi u +(1-\phi)u) * (\psi v + (1-\psi)v) \\
    &= \phi u * \psi v + \phi u * (1-\psi)v + (1-\phi)u * \psi v + (1-\phi)u * (1-\psi)v
\end{flalign}

All but the first two terms above are $C^\infty$ therefore,

\begin{flalign}
    \singsupp u*v &= \singsupp \phi u * \psi v \\
    &\subset \supp \phi u * \psi v \subset \supp \phi u + \supp \psi v \subset \supp \phi + \supp \psi
\end{flalign}

Now if we consider all admissable $\phi$ and $\psi$,

\begin{flalign}
    \singsupp u*v \subset \bigcap_{\phi | \phi = 1, \psi | \psi = 1 \text{ on } \singsupp u, v \text{ resp.}} \supp \phi + \supp \psi = \singsupp u + \singsupp v
\end{flalign}


\subsubsection*{Example}
On $\reals$, $u = \delta(x)$, $v = \delta(x-1)$

\begin{flalign}
    u*v = \delta(x) *v = v \delta(x-1)
\end{flalign}

we know $\singsupp \delta = \{ 0 \}$ and $\singsupp \delta(x-1) = \{ 1 \}$, therefore we see that the lemma applies.

\section{Elliptic Differential Operators}

Let,

\begin{equation}
    P(-i\partial) = \sum_{|\alpha| \leq m} = a_\alpha (-i\partial)^\alpha
\end{equation}

be linear differential operator of order $m$, with constant coefficients $a_\alpha \in \complexes$.

\begin{definition}
    The polynomial,
    \[ P(\zeta) = \sum_{|\alpha| \leq m} = a_\alpha \zeta^\alpha\]
    is called the \textbf{symbol} of $P$. The polynomial,
    \[ \sigma_P(\zeta) = \sum_{|\alpha| = m} a_\alpha \zeta^\alpha \]
    is called the \textbf{principle symbol} of $P$
\end{definition}

\begin{definition}
    $P$ is called elliptic if,
    \[ \sigma_P (\zeta) \neq 0 \text{ for } \zeta \neq 0 \]
\end{definition}

\begin{theorem}[Elliptic Regularity Theorem]
    Let $P$ be an elliptic operator with constant coefficients of order $m$, then,
    \[ \singsupp u = \singsupp Pu \]
    $\forall u \in \dist$
\end{theorem}

This basically says that the singularities are unmoved under the application of an elliptic differentail operator.

\subsubsection*{Proof}

\subsubsection*{1. Forward inclusion}

Consider,

\begin{flalign}
    \singsupp Pu &\subset \singsupp u \Leftrightarrow \\
    \overline{\singsupp u}^c &\subset \overline{\singsupp Pu}^c
\end{flalign}

This statement must be true, and is obvious, since if $u \in C^\infty(B_R(x_0))$  then $Pu \in  C^\infty(B_R(x_0))$ also.


\subsubsection*{2. Reverse inclusion}

Let's now show that,

\begin{equation}
    \singsupp u \subset \singsupp Pu
\end{equation}

thus verifying our theorem. From the elliptic nature of $P$, we have that,

\begin{flalign}
    \sigma_P (\zeta) \neq 0 \text{ for } \zeta \neq 0
\end{flalign}

Therefore, $|\sigma_P(\zeta) | > 0$ for some set $\zeta \in S^1$. Furthermore,  $|\sigma_P(\zeta) | > C > 0$ for the unit sphere $\zeta \in S^1$. The unit sphere is a compacy set, the principle symbol is a homogenous function of order $N$, therefore

\begin{flalign}
    |\sigma_P(\zeta) | > C |\zeta|^m, \> \> \> \forall \zeta \in \reals^N
\end{flalign}

in this context homogenous means that $\sigma_P(t_\zeta) = t^m \sigma_P(\zeta)$ where $t \geq 0$. Therefore,

\begin{flalign}
    |P(\zeta)| \geq C |\zeta|^m - C_1 |\zeta|^{m-1} - .... C_m
\end{flalign}

with some some constants $C_j$. Therefore, for constant $\forall M > 0 $ one can find $R$ such that $|P(\zeta)| > M$ for $|\zeta| > R$. Let $\xi \in D$ such that $0 \leq \xi 1$ and $\xi = 1$ for $|\zeta| < R$, then $\frac{1-\xi(\zeta)}{|P(\zeta|} \leq \frac{1}{M}$ $\forall \zeta$. Therefore we can say that $\frac{1-\xi(\zeta)}{P(\zeta)} \in S'$ as it is a continuous function, and bounded. Denote its inverse Fourier transform by $E$. Thus,

\begin{flalign}
    \hat{E} = \frac{1- \xi(\zeta)}{P(\zeta)}
\end{flalign}

Asssume for the moment that $E \in C^\infty(\reals^N \backslash 0)$. Then as $E \in S'$ one has the following identities,

\begin{flalign}
    \widehat{P(-i\partial)E} \overset{\text{op. rules}}{=} P(\zeta) \hat{E} = P(\zeta)\frac{1-\xi(\zeta)}{P(\zeta)} = 1-\xi(\zeta) \\
    \implies P(-i\partial)E = \delta(x) - \mathcal{F}^{-1}\xi(\zeta)
\end{flalign}

The point is that $\mathcal{F}^{-1}\xi(\zeta) \in C^\infty$ as $\xi \in D \subset S$, and except for this final term we see that $E$ is almost a fundamental solution. Let $\Omega$ be a compact set in $\reals^N$. Choose $\psi \in \tfspaceD$ such that $\psi = 1$ on $\Omega$ then $\forall u \in \dist$

\begin{flalign}
    \psi u &= \delta * \psi u = \left [ P(-i\partial) + \mathcal{F}^{-1}\xi \right] * \psi u \\
    & =  P(-i\partial) *\psi u+ \mathcal{F}^{-1}\xi * \psi u \\
    &\overset{\text{diff. of conv.}}{=} E* [ P(-i\delta)\psi u] + \underset{\in C^\infty}{\mathcal{F}^{-1}\xi} * \underset{\in \dist, \text{ compact supp. so } \in C^\infty}{\psi u}
\end{flalign}

Therefore,

\begin{flalign}
    \singsupp \psi u = \singsupp E *  [P(-i\partial) \psi u]
\end{flalign}

By lemma of singular support of convolution,

\begin{flalign}
    \singsupp \psi u \subset \singsupp E + \singsupp P\psi u
\end{flalign}

By assumption, $\singsupp E = \{0\}$, consequently

\begin{flalign}
    \singsupp{ \psi u} \subset \singsupp P\psi u
\end{flalign}

Since $\Omega$ is any compact set in $\reals^N$, we get,

\begin{flalign}
    \singsupp u \subset \singsupp P u
\end{flalign}

Finally check our assumption about the $\singsupp E = \{ 0\}$. Indeed,

\begin{flalign}
    | \mathcal{F} [x^\beta \partial^\alpha E] | \overset{\text{op. rules}}{=} |\partial^\beta \zeta^\alpha \hat{E}|, \> \> \> \forall \alpha, \beta
\end{flalign}

by calculate we see that,

\begin{flalign}
    |\partial^{\beta} (\zeta^\alpha \hat{E})| = O(|\zeta|^{-M+|\alpha|-|\beta|})
\end{flalign}

as $|\zeta| \rightarrow +\infty$. Choosing $|\beta| = |\alpha| + N -M+1$, we get,

\begin{flalign}
    \mathcal{F} [x^\beta \partial^\alpha E] \in \lone
\end{flalign}

Therefore,

\begin{flalign}
    x^\beta \partial^\alpha E \in C(\reals^N)
\end{flalign}

if $|\beta| = |\alpha| + N -M+1$, indeed,

\begin{flalign}
    \partial^\alpha E \in C(\reals^N \backslash 0), \> \> \forall \alpha \\
    \implies E \in C(\reals^N \backslash 0)
\end{flalign}

\section{Fourier Transform on $L_2$}

\section{Sobolev Spaces $H^s(\reals^N)$}

\end{document}